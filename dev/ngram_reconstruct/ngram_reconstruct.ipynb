{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from collections import OrderedDict\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER = 2\n",
    "NUM_WORDS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Preprocessing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "def extract_ngrams(vocab, sent, order):\n",
    "    ngrams = []\n",
    "    \n",
    "    # tokenization\n",
    "    uwords = [t.text for t in nlp(str(sent))]\n",
    "    \n",
    "    # extract ngrams\n",
    "    for oo in range(1, order + 1):\n",
    "        for ng in set([' '.join(t).strip() for t in zip(*[uwords[i:] for i in range(oo)])]):\n",
    "            ngrams.append(ng)\n",
    "            if ng in vocab:\n",
    "                vocab[ng] += 1\n",
    "            else:\n",
    "                vocab[ng] = 1\n",
    "\n",
    "\n",
    "    return vocab, ngrams\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, order):\n",
    "        self.name = name\n",
    "        \n",
    "        # for single words\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "        \n",
    "        # for ngrams\n",
    "        self.order = order\n",
    "        self.vocab0 = OrderedDict()\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "        self.vocab0, ngrams = extract_ngrams(self.vocab0, sentence, self.order)\n",
    "        return ngrams\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def createNGramDictionary(self):\n",
    "        tokens = list(self.vocab0.keys())\n",
    "        freqs = list(self.vocab0.values())\n",
    "        sidx = np.argsort(freqs)[::-1]\n",
    "        vocab = OrderedDict([(tokens[s], i) for i, s in enumerate(sidx)])\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, order, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2, order)\n",
    "        output_lang = Lang(lang1, order)\n",
    "    else:\n",
    "        input_lang = Lang(lang1, order)\n",
    "        output_lang = Lang(lang2, order)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 7\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 5412 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 1644\n",
      "[['he', 'my', '.', 'hero', 's', 'my hero', 'hero .', 'he s', 's my'], 'he s my hero .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, order, reverse=False):\n",
    "    data = []\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, order, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        #input_lang.addSentence(pair[0])\n",
    "        pair[0] = output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "#     print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', ORDER, True)\n",
    "vocab_ngrams = output_lang.createNGramDictionary()\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6711"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5412\n",
      "[['happy', 'more', 'than', '.', 'm', 'i', 'more than', 'm more', 'happy .', 'i m', 'than happy'], 'i m more than happy .']\n",
      "[['they', 're', 'asleep', '.', 'they re', 'asleep .', 're asleep'], 'they re asleep .']\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(pairs)\n",
    "print(len(pairs))\n",
    "train_set = pairs[:5120]\n",
    "test_set = pairs[5120:]\n",
    "print(random.choice(train_set))\n",
    "print(random.choice(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" â†’ \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector â€” a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "#     def forward(self, input, hidden):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         output = embedded\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "#         if use_cuda:\n",
    "#             return result.cuda()\n",
    "#         else:\n",
    "#             return result\n",
    "\n",
    "class NGramEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NGramEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        hidden = hidden + embedded\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH*ORDER):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def indexesFromNGramList(vocab, ngram_list, order, num_words):\n",
    "    result = []\n",
    "    for ng in ngram_list:\n",
    "        if ng in vocab:\n",
    "            idx = vocab[ng]\n",
    "            if idx > num_words:\n",
    "                pass\n",
    "            else:\n",
    "                result.append(idx)\n",
    "        else:\n",
    "            pass\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def variableFromNGramList(vocab, ngram_list, order, num_words):\n",
    "    indexes = indexesFromNGramList(vocab, ngram_list, order, num_words)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "    \n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromNGramList(vocab_ngrams, pair[0], ORDER, NUM_WORDS)\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 'they', 'big', 'are', 'very', 'big .', 'very big', 'are very', 'they are'], 'they are very big .']\n",
      "(Variable containing:\n",
      "    0\n",
      "   15\n",
      "  105\n",
      "   18\n",
      "   11\n",
      "  213\n",
      "  878\n",
      "   86\n",
      "   42\n",
      "[torch.LongTensor of size 9x1]\n",
      ", Variable containing:\n",
      " 210\n",
      " 122\n",
      " 282\n",
      " 129\n",
      "   4\n",
      "   1\n",
      "[torch.LongTensor of size 6x1]\n",
      ")\n",
      "[2, 3, 12, 4]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[100])\n",
    "print(variablesFromPair(train_set[100]))\n",
    "print(indexesFromSentence(output_lang, 'i m shy .'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH*ORDER):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(train_set))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH*ORDER):\n",
    "    input_variable = variableFromNGramList(vocab_ngrams, sentence, ORDER, NUM_WORDS)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "#         decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#             decoder_input, decoder_hidden, encoder_outputs)\n",
    "#         decoder_attentions[di] = decoder_attention.data\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "#     return decoded_words, decoder_attentions[:di + 1]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(train_set)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "#         output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 256\n",
    "# encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1)\n",
    "\n",
    "# if use_cuda:\n",
    "#     encoder1 = encoder1.cuda()\n",
    "#     attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "\n",
    "hidden_size = 100\n",
    "encoder1 = NGramEncoder(NUM_WORDS, hidden_size)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['prepared', '.', 'i', 'm', 'prepared .', 'm prepared', 'i m']\n",
      "= i m prepared .\n",
      "< blackmailing blackmailing smartly smartly smartly smartly smartly smartly smartly smartly smartly smartly smartly smartly\n",
      "\n",
      "> ['complete', 'idiot', '.', 'm', 'i', 'a', 'm a', 'a complete', 'i m', 'idiot .', 'complete idiot']\n",
      "= i m a complete idiot .\n",
      "< worn worn approaching bald bald courageous abusing abusing horrible them starting starting bit smiling\n",
      "\n",
      "> ['.', 'm', 'i', 'without', 'you', 'sad', 'without you', 'you .', 'i m', 'sad without', 'm sad']\n",
      "= i m sad without you .\n",
      "< disgusted disgusted hearing hearing hearing hearing hearing books books tycoon savage bit bit sweat\n",
      "\n",
      "> ['are', 'you', '.', 'morons', 'are morons', 'morons .', 'you are']\n",
      "= you are morons .\n",
      "< lies startled consciousness startled heating heating babies smiling sweat entirely smiling sweat entirely smiling\n",
      "\n",
      "> ['.', 'you', 're', 'bright', 're bright', 'you re', 'bright .']\n",
      "= you re bright .\n",
      "< comfortable comfortable silent comfortable after headed headed headed headed headed headed headed about crazy\n",
      "\n",
      "> ['.', 'm', 'i', 'getting', 'sleepy', 'sleepy .', 'm getting', 'i m', 'getting sleepy']\n",
      "= i m getting sleepy .\n",
      "< successful successful successful than than luck luck luck luck luck luck luck luck luck\n",
      "\n",
      "> ['.', 'you', 're', 'trustworthy', 'trustworthy .', 're trustworthy', 'you re']\n",
      "= you re trustworthy .\n",
      "< find find find find find once starting starting starting bit smiling sweat sweat callous\n",
      "\n",
      "> ['to', '.', 'work', 'm', 'i', 'going', 'i m', 'm going', 'work .', 'to work', 'going to']\n",
      "= i m going to work .\n",
      "< precise extroverted extroverted tough prisoners contented smartly smartly smartly smartly smartly smartly smartly smartly\n",
      "\n",
      "> ['uninsured', '.', 'i', 'm', 'm uninsured', 'i m', 'uninsured .']\n",
      "= i m uninsured .\n",
      "< color class class wealthy class under starting starting starting bit starting bit smiling sweat\n",
      "\n",
      "> ['m', 'blushing', 'i', '!', 'not', 'not blushing', 'i m', 'm not', 'blushing !']\n",
      "= i m not blushing !\n",
      "< productive killing direct killing direct starting bit starting bit bit smiling sweat sweat callous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 22s (- 18m 23s) (1000 2%) 2.5158\n",
      "0m 45s (- 18m 3s) (2000 4%) 1.9601\n",
      "1m 7s (- 17m 35s) (3000 6%) 1.7672\n",
      "1m 30s (- 17m 23s) (4000 8%) 1.5550\n",
      "1m 50s (- 16m 37s) (5000 10%) 1.2841\n",
      "2m 12s (- 16m 9s) (6000 12%) 1.1711\n",
      "2m 34s (- 15m 47s) (7000 14%) 1.0302\n",
      "2m 53s (- 15m 11s) (8000 16%) 0.9683\n",
      "3m 14s (- 14m 45s) (9000 18%) 0.8209\n",
      "3m 37s (- 14m 30s) (10000 20%) 0.7404\n",
      "4m 1s (- 14m 17s) (11000 22%) 0.6691\n",
      "4m 23s (- 13m 54s) (12000 24%) 0.6330\n",
      "4m 47s (- 13m 37s) (13000 26%) 0.5311\n",
      "5m 8s (- 13m 13s) (14000 28%) 0.5252\n",
      "5m 31s (- 12m 53s) (15000 30%) 0.4733\n",
      "5m 54s (- 12m 32s) (16000 32%) 0.4581\n",
      "6m 14s (- 12m 7s) (17000 34%) 0.3950\n",
      "6m 37s (- 11m 46s) (18000 36%) 0.4076\n",
      "6m 57s (- 11m 21s) (19000 38%) 0.3467\n",
      "7m 18s (- 10m 58s) (20000 40%) 0.2900\n",
      "7m 42s (- 10m 39s) (21000 42%) 0.3001\n",
      "8m 4s (- 10m 16s) (22000 44%) 0.2614\n",
      "8m 24s (- 9m 52s) (23000 46%) 0.2372\n",
      "8m 46s (- 9m 30s) (24000 48%) 0.2313\n",
      "9m 6s (- 9m 6s) (25000 50%) 0.2250\n",
      "9m 25s (- 8m 42s) (26000 52%) 0.1890\n",
      "9m 45s (- 8m 19s) (27000 54%) 0.1667\n",
      "10m 9s (- 7m 58s) (28000 56%) 0.1679\n",
      "10m 29s (- 7m 35s) (29000 57%) 0.1622\n",
      "10m 50s (- 7m 13s) (30000 60%) 0.1308\n",
      "11m 11s (- 6m 51s) (31000 62%) 0.1154\n",
      "11m 33s (- 6m 29s) (32000 64%) 0.1294\n",
      "11m 52s (- 6m 7s) (33000 66%) 0.1129\n",
      "12m 12s (- 5m 44s) (34000 68%) 0.1032\n",
      "12m 33s (- 5m 22s) (35000 70%) 0.1042\n",
      "12m 52s (- 5m 0s) (36000 72%) 0.1121\n",
      "13m 12s (- 4m 38s) (37000 74%) 0.0863\n",
      "13m 32s (- 4m 16s) (38000 76%) 0.0897\n",
      "13m 51s (- 3m 54s) (39000 78%) 0.0711\n",
      "14m 11s (- 3m 32s) (40000 80%) 0.0707\n",
      "14m 33s (- 3m 11s) (41000 82%) 0.0553\n",
      "14m 55s (- 2m 50s) (42000 84%) 0.0541\n",
      "15m 17s (- 2m 29s) (43000 86%) 0.0456\n",
      "15m 40s (- 2m 8s) (44000 88%) 0.0510\n",
      "16m 1s (- 1m 46s) (45000 90%) 0.0564\n",
      "16m 21s (- 1m 25s) (46000 92%) 0.0500\n",
      "16m 45s (- 1m 4s) (47000 94%) 0.0550\n",
      "17m 6s (- 0m 42s) (48000 96%) 0.0460\n",
      "17m 27s (- 0m 21s) (49000 98%) 0.0366\n",
      "17m 48s (- 0m 0s) (50000 100%) 0.0458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1289d53c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1289bb198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, 50000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['.', 'i', 'stunned', 'm', 'm stunned', 'i m', 'stunned .']\n",
      "= i m stunned .\n",
      "< i m stunned . <EOS>\n",
      "\n",
      "> ['nuts', 'you', 're', '!', 're nuts', 'you re', 'nuts !']\n",
      "= you re nuts !\n",
      "< you re nuts ! <EOS>\n",
      "\n",
      "> ['.', 'chubby', 'i', 'm', 'chubby .', 'm chubby', 'i m']\n",
      "= i m chubby .\n",
      "< i m chubby . <EOS>\n",
      "\n",
      "> ['.', 'i', 'concerned', 'm', 'i m', 'concerned .', 'm concerned']\n",
      "= i m concerned .\n",
      "< i m concerned . <EOS>\n",
      "\n",
      "> ['working', 'here', '.', 'm', 'i', 'working here', 'here .', 'i m', 'm working']\n",
      "= i m working here .\n",
      "< i m working here . <EOS>\n",
      "\n",
      "> ['the', '.', 'm', 'calling', 'cops', 'i', 'm calling', 'i m', 'cops .', 'the cops', 'calling the']\n",
      "= i m calling the cops .\n",
      "< i m calling the cops . <EOS>\n",
      "\n",
      "> ['happy', 're', 'all', '.', 'you', 'happy .', 're all', 'you re', 'all happy']\n",
      "= you re all happy .\n",
      "< you re all happy . <EOS>\n",
      "\n",
      "> ['.', 'thinking', 'i', 'm', 'm thinking', 'i m', 'thinking .']\n",
      "= i m thinking .\n",
      "< i m thinking . <EOS>\n",
      "\n",
      "> ['bad', 'you', 're', '.', 're bad', 'you re', 'bad .']\n",
      "= you re bad .\n",
      "< you re bad . <EOS>\n",
      "\n",
      "> ['bad', '.', 'a', 'are', 'you', 'person', 'are a', 'you are', 'bad person', 'a bad', 'person .']\n",
      "= you are a bad person .\n",
      "< you are a bad person . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i m hungry . <EOS>\n",
      "we re the married . <EOS>\n",
      "they re running . <EOS>\n",
      "they re the . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def ngram_extractor(sent, order=ORDER):\n",
    "    ngrams = []\n",
    "    \n",
    "    # tokenization\n",
    "    uwords = [t.text for t in nlp(str(sent))]\n",
    "    \n",
    "    # extract ngrams\n",
    "    for oo in range(1, order + 1):\n",
    "        for ng in set([' '.join(t).strip() for t in zip(*[uwords[i:] for i in range(oo)])]):\n",
    "            ngrams.append(ng)\n",
    "            \n",
    "    return ngrams\n",
    "\n",
    "output_words = evaluate(encoder1, decoder1, ngram_extractor(\"i m hungry\"))\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)\n",
    "\n",
    "output_words = evaluate(encoder1, decoder1, ngram_extractor(\"we run the input sentence through the encoder .\"))\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)\n",
    "\n",
    "output_words = evaluate(encoder1, decoder1, ngram_extractor(\"they re running fast .\"))\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)\n",
    "\n",
    "output_words = evaluate(encoder1, decoder1, ngram_extractor(\"they re winning the game.\"))\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract as list, not set\n",
    "def ngram_extractor_eval(sent, order=1):\n",
    "    ngrams = []\n",
    "    \n",
    "    # tokenization\n",
    "    uwords = [t.text for t in nlp(str(sent))]\n",
    "    \n",
    "    # extract ngrams\n",
    "    for oo in range(1, order + 1):\n",
    "        for ng in ([' '.join(t).strip() for t in zip(*[uwords[i:] for i in range(oo)])]):\n",
    "            ngrams.append(ng)\n",
    "            \n",
    "    return ngrams\n",
    "\n",
    "\n",
    "#ROUGE Score, match/reference length\n",
    "def ROUGE(cand, ref, n=ORDER):\n",
    "    cand_ngrams = ngram_extractor_eval(cand, n)\n",
    "    ref_ngrams = ngram_extractor_eval(ref, n)\n",
    "    count = 0\n",
    "    for gram in ref_ngrams:\n",
    "        if gram in cand_ngrams:\n",
    "            count += 1\n",
    "    return count/len(cand_ngrams)\n",
    "\n",
    "#BLEU Score, match/candidate length, without clipping\n",
    "def BLEU(cand, ref, n=ORDER):\n",
    "    cand_ngrams = ngram_extractor_eval(cand, n)\n",
    "    ref_ngrams = ngram_extractor_eval(ref, n)\n",
    "    count = 0\n",
    "    for gram in cand_ngrams:\n",
    "        if gram in ref_ngrams:\n",
    "            count += 1\n",
    "    return count/len(ref_ngrams)\n",
    "\n",
    "def BLEU_clip(cand, ref, n=ORDER):\n",
    "    cand_ngrams = ngram_extractor_eval(cand, n)\n",
    "    ref_ngrams = ngram_extractor_eval(ref, n)\n",
    "    l = len(ref_ngrams)\n",
    "    count = 0\n",
    "    for gram in cand_ngrams:\n",
    "        if gram in ref_ngrams:\n",
    "            count += 1\n",
    "            ref_ngrams.remove(gram)\n",
    "    return count/l\n",
    "\n",
    "#Coherence score is used for paragraphs, not used yet\n",
    "def Coherence():\n",
    "    pass\n",
    "\n",
    "def score(list_cand, list_ref, n=ORDER, method='ROUGE'):\n",
    "    score = 0\n",
    "    dic = {'ROUGE':ROUGE, 'BLEU':BLEU, 'BLEU_clip':BLEU_clip}\n",
    "    fun = dic[method]\n",
    "    num_sent = len(list_cand)\n",
    "    for i in range(num_sent):\n",
    "        score += fun(list_cand[i], list_ref[i], n)\n",
    "    return score/num_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomlyWithScore(encoder, decoder, n=10, method=\"ROUGE\"):\n",
    "    list_cand = []\n",
    "    list_ref = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(test_set)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "#         output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        list_cand.append(output_sentence)\n",
    "        list_ref.append(pair[1] + \" <EOS>\")\n",
    "    print(score(list_cand, list_ref, method=method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['he', 'asleep', 'sound', '.', 's', 'sound asleep', 's sound', 'he s', 'asleep .']\n",
      "= he s sound asleep .\n",
      "< he s sound asleep . <EOS>\n",
      "\n",
      "> ['.', 'm', 'i', 'satisfied', 'not', 'not satisfied', 'satisfied .', 'i m', 'm not']\n",
      "= i m not satisfied .\n",
      "< i m not satisfied . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'reliable', 're reliable', 'you re', 'reliable .']\n",
      "= you re reliable .\n",
      "< you re reliable . <EOS>\n",
      "\n",
      "> ['he', 'asleep', 'sound', '.', 's', 'sound asleep', 's sound', 'he s', 'asleep .']\n",
      "= he s sound asleep .\n",
      "< he s sound asleep . <EOS>\n",
      "\n",
      "> ['exam', 'the', '.', 'm', 'i', 'dreading', 'dreading the', 'i m', 'm dreading', 'exam .', 'the exam']\n",
      "= i m dreading the exam .\n",
      "< i m dreading the exam . <EOS>\n",
      "\n",
      "> ['.', 'm', 'i', 'a', 'psychic', 'not', 'not a', 'i m', 'psychic .', 'a psychic', 'm not']\n",
      "= i m not a psychic .\n",
      "< i m not a psychic . <EOS>\n",
      "\n",
      "> ['exhausted', 'so', 'i', 'am', '!', 'so exhausted', 'exhausted !', 'am so', 'i am']\n",
      "= i am so exhausted !\n",
      "< i am so exhausted ! <EOS>\n",
      "\n",
      "> ['tough', '.', 'i', 'm', 'm tough', 'i m', 'tough .']\n",
      "= i m tough .\n",
      "< i m tough . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'overreacting', 'overreacting .', 'you re', 're overreacting']\n",
      "= you re overreacting .\n",
      "< you re overreacting . <EOS>\n",
      "\n",
      "> ['cat', '.', 'i', 'a', 'am', 'person', 'i am', 'cat person', 'a cat', 'am a', 'person .']\n",
      "= i am a cat person .\n",
      "< i am a cat person . <EOS>\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomlyWithScore(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateDatasetWithScore(encoder, decoder, method=\"ROUGE\"):\n",
    "    list_cand = []\n",
    "    list_ref = []\n",
    "    for i in range(len(test_set)):\n",
    "        pair = test_set[i]\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "#         output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        list_cand.append(output_sentence)\n",
    "        list_ref.append(pair[1] + \" <EOS>\")\n",
    "    return score(list_cand, list_ref, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['absolutely', '.', 'right', 'are', 'you', 'are absolutely', 'absolutely right', 'you are', 'right .']\n",
      "= you are absolutely right .\n",
      "< you are absolutely right . <EOS>\n",
      "\n",
      "> ['re', 'schedule', '.', 'we', 'behind', 'behind schedule', 'we re', 're behind', 'schedule .']\n",
      "= we re behind schedule .\n",
      "< we re really . <EOS>\n",
      "\n",
      "> ['already', '.', 'm', 'i', 'married', 'married .', 'i m', 'm already', 'already married']\n",
      "= i m already married .\n",
      "< i m already married . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'smart', 'very', 'very smart', 'smart .', 'you re', 're very']\n",
      "= you re very smart .\n",
      "< you re very smart . <EOS>\n",
      "\n",
      "> ['timid', 're', 'you', '.', 'very', 'you re', 'timid .', 'very timid', 're very']\n",
      "= you re very timid .\n",
      "< you re very timid . <EOS>\n",
      "\n",
      "> ['he', 'traveling', 'to', '.', 's', 'accustomed', 'to traveling', 'traveling .', 'accustomed to', 's accustomed', 'he s']\n",
      "= he s accustomed to traveling .\n",
      "< he s buying to . <EOS>\n",
      "\n",
      "> ['.', 'm', 'i', 'still', 'married', 'm still', 'still married', 'i m', 'married .']\n",
      "= i m still married .\n",
      "< i m still married . <EOS>\n",
      "\n",
      "> ['re', 'luck', '.', 'you', 'in', 're in', 'you re', 'luck .', 'in luck']\n",
      "= you re in luck .\n",
      "< you re in luck . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'big', 'big .', 'you re', 're big']\n",
      "= you re big .\n",
      "< you re big . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'thin', 're thin', 'you re', 'thin .']\n",
      "= you re thin .\n",
      "< you re thin . <EOS>\n",
      "\n",
      "> ['friendly', 'seems', '.', 'she', 'friendly .', 'she seems', 'seems friendly']\n",
      "= she seems friendly .\n",
      "< she seems fat . <EOS>\n",
      "\n",
      "> ['re', 'too', '.', 'you', 'polite', 'you re', 're too', 'too polite', 'polite .']\n",
      "= you re too polite .\n",
      "< you re too polite . <EOS>\n",
      "\n",
      "> ['rational', '.', 'i', 'm', 'rational .', 'm rational', 'i m']\n",
      "= i m rational .\n",
      "< i m rational . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'half', 'right', 'right .', 'we re', 're half', 'half right']\n",
      "= we re half right .\n",
      "< we re half . <EOS>\n",
      "\n",
      "> ['extroverted', 'you', 're', '.', 'extroverted .', 'you re', 're extroverted']\n",
      "= you re extroverted .\n",
      "< you re extroverted . <EOS>\n",
      "\n",
      "> ['.', 'i', 'm', 'sure', 'sure .', 'i m', 'm sure']\n",
      "= i m sure .\n",
      "< i m sure . <EOS>\n",
      "\n",
      "> ['re', 'teacher', 'the', '.', 'you', 're the', 'teacher .', 'you re', 'the teacher']\n",
      "= you re the teacher .\n",
      "< you re the teacher . <EOS>\n",
      "\n",
      "> ['.', 'i', 'foreigner', 'a', 'am', 'am a', 'i am', 'foreigner .', 'a foreigner']\n",
      "= i am a foreigner .\n",
      "< i am a foreigner . <EOS>\n",
      "\n",
      "> ['.', 'they', 'are', 'tired', 'not', 'are not', 'tired .', 'not tired', 'they are']\n",
      "= they are not tired .\n",
      "< they are not tired . <EOS>\n",
      "\n",
      "> ['.', 's', 'he', 'asleep', 's asleep', 'asleep .', 'he s']\n",
      "= he s asleep .\n",
      "< he s asleep . <EOS>\n",
      "\n",
      "> ['re', '.', 'awesome', 'really', 'you', 'awesome .', 'you re', 're really', 'really awesome']\n",
      "= you re really awesome .\n",
      "< you re really awesome . <EOS>\n",
      "\n",
      "> ['just', '.', 'm', 'i', 'tired', 'm just', 'just tired', 'i m', 'tired .']\n",
      "= i m just tired .\n",
      "< i m just tired . <EOS>\n",
      "\n",
      "> ['feeling', '.', 'm', 'i', 'better', 'much', 'feeling much', 'i m', 'm feeling', 'much better', 'better .']\n",
      "= i m feeling much better .\n",
      "< i m feeling feeling better . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'sleepy', 'sleepy .', 'you re', 're sleepy']\n",
      "= you re sleepy .\n",
      "< you re sleepy . <EOS>\n",
      "\n",
      "> ['happy', '.', 'm', 'i', 'not', 'happy .', 'i m', 'm not', 'not happy']\n",
      "= i m not happy .\n",
      "< i m not happy . <EOS>\n",
      "\n",
      "> ['disposal', '.', 'm', 'i', 'at', 'your', 'your disposal', 'i m', 'disposal .', 'at your', 'm at']\n",
      "= i m at your disposal .\n",
      "< i m at your disposal . <EOS>\n",
      "\n",
      "> ['.', 'embarrassed', 'm', 'i', 'not', 'not embarrassed', 'i m', 'm not', 'embarrassed .']\n",
      "= i m not embarrassed .\n",
      "< i m not embarrassed . <EOS>\n",
      "\n",
      "> ['.', 'wasted', 'i', 'm', 'm wasted', 'i m', 'wasted .']\n",
      "= i m wasted .\n",
      "< i m annoying . <EOS>\n",
      "\n",
      "> ['he', 'short', '.', 'is', 'strong', 'but', 'short but', 'but strong', 'he is', 'is short', 'strong .']\n",
      "= he is short but strong .\n",
      "< he is not kind . <EOS>\n",
      "\n",
      "> ['order', 're', 'you', '.', 'out', 'of', 'you re', 'out of', 'order .', 'of order', 're out']\n",
      "= you re out of order .\n",
      "< you re out of girl . <EOS>\n",
      "\n",
      "> ['exhausted', 'so', 'i', 'am', '!', 'so exhausted', 'exhausted !', 'am so', 'i am']\n",
      "= i am so exhausted !\n",
      "< i am so exhausted ! <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'perfect', 'not', 'we re', 'perfect .', 'not perfect', 're not']\n",
      "= we re not perfect .\n",
      "< we re not perfect . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'curious', 'very', 'curious .', 'you re', 're very', 'very curious']\n",
      "= you re very curious .\n",
      "< you re very curious . <EOS>\n",
      "\n",
      "> ['re', '.', 'joking', 'course', 'you', 'of', 're joking', 'you re', 'of course', 'joking of', 'course .']\n",
      "= you re joking of course .\n",
      "< you re joking of course . <EOS>\n",
      "\n",
      "> ['tall', '.', 'i', 'am', 'very', 'am very', 'tall .', 'i am', 'very tall']\n",
      "= i am very tall .\n",
      "< i am very tall . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'jealous', 'you re', 're jealous', 'jealous .']\n",
      "= you re jealous .\n",
      "< you re jealous . <EOS>\n",
      "\n",
      "> ['amazing', 'you', 're', '.', 're amazing', 'you re', 'amazing .']\n",
      "= you re amazing .\n",
      "< you re amazing . <EOS>\n",
      "\n",
      "> ['re', 'me', '.', 'behind', 'they', 'right', 'behind me', 'me .', 'right behind', 're right', 'they re']\n",
      "= they re right behind me .\n",
      "< they re right behind me . <EOS>\n",
      "\n",
      "> ['kidding', '.', 'm', 'i', 'not', 'kidding .', 'not kidding', 'i m', 'm not']\n",
      "= i m not kidding .\n",
      "< i m not kidding . <EOS>\n",
      "\n",
      "> ['model', '.', 'she', 's', 'a', 'a model', 'she s', 'model .', 's a']\n",
      "= she s a model .\n",
      "< she s a model . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'very', 'brave', 'very brave', 'you re', 'brave .', 're very']\n",
      "= you re very brave .\n",
      "< you re very brave . <EOS>\n",
      "\n",
      "> ['english', '.', 'm', 'i', 'not', 'english .', 'i m', 'm not', 'not english']\n",
      "= i m not english .\n",
      "< i m not english . <EOS>\n",
      "\n",
      "> ['always', '.', 'm', 'thirsty', 'i', 'm always', 'always thirsty', 'thirsty .', 'i m']\n",
      "= i m always thirsty .\n",
      "< i m always thirsty . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'annoying', 'annoying .', 're annoying', 'you re']\n",
      "= you re annoying .\n",
      "< you re annoying . <EOS>\n",
      "\n",
      "> ['happy', 'more', 'than', '.', 'm', 'i', 'more than', 'm more', 'happy .', 'i m', 'than happy']\n",
      "= i m more than happy .\n",
      "< i m more than happy . <EOS>\n",
      "\n",
      "> ['just', 'kidding', '.', 'm', 'i', 'm just', 'kidding .', 'i m', 'just kidding']\n",
      "= i m just kidding .\n",
      "< i m just kidding . <EOS>\n",
      "\n",
      "> ['available', 'he', '.', 's', 'not', 'available .', 'not available', 'he s', 's not']\n",
      "= he s not available .\n",
      "< he s not available . <EOS>\n",
      "\n",
      "> ['for', '.', 'work', 'm', 'i', 'looking', 'm looking', 'i m', 'for work', 'work .', 'looking for']\n",
      "= i m looking for work .\n",
      "< i m looking . <EOS>\n",
      "\n",
      "> ['timid', 're', 'you', '.', 'very', 'you re', 'timid .', 'very timid', 're very']\n",
      "= you re very timid .\n",
      "< you re very timid . <EOS>\n",
      "\n",
      "> ['re', 'just', 'for', '.', 'they', 'you', 'for you', 'just for', 'you .', 'they re', 're just']\n",
      "= they re just for you .\n",
      "< they re just for you . <EOS>\n",
      "\n",
      "> ['completely', 're', '.', 'right', 'you', 'right .', 'you re', 'completely right', 're completely']\n",
      "= you re completely right .\n",
      "< you re completely right . <EOS>\n",
      "\n",
      "> ['speaking', 're', 'french', '.', 'they', 're speaking', 'they re', 'speaking french', 'french .']\n",
      "= they re speaking french .\n",
      "< they re speaking french . <EOS>\n",
      "\n",
      "> ['re', 'girl', 'an', '.', 'interesting', 'you', 'girl .', 're an', 'you re', 'interesting girl', 'an interesting']\n",
      "= you re an interesting girl .\n",
      "< you re an interesting girl . <EOS>\n",
      "\n",
      "> ['to', '.', 'm', 'start', 'i', 'ready', 'to start', 'm ready', 'i m', 'ready to', 'start .']\n",
      "= i m ready to start .\n",
      "< i m ready to him . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'old', 'you re', 're old', 'old .']\n",
      "= you re old .\n",
      "< you re old . <EOS>\n",
      "\n",
      "> ['they', 'babies', 're', '.', 'they re', 're babies', 'babies .']\n",
      "= they re babies .\n",
      "< they re probably . <EOS>\n",
      "\n",
      "> ['desperate', '.', 'i', 'm', 'desperate .', 'i m', 'm desperate']\n",
      "= i m desperate .\n",
      "< i m desperate . <EOS>\n",
      "\n",
      "> ['re', 'late', 'you', '.', 'very', 'you re', 'very late', 're very', 'late .']\n",
      "= you re very late .\n",
      "< you re very efficient . <EOS>\n",
      "\n",
      "> ['is', '.', 'french', 'she', 'is french', 'she is', 'french .']\n",
      "= she is french .\n",
      "< she is fond french . <EOS>\n",
      "\n",
      "> ['genius', '.', 'a', 'are', 'you', 'are a', 'genius .', 'you are', 'a genius']\n",
      "= you are a genius .\n",
      "< you are a genius . <EOS>\n",
      "\n",
      "> ['my', '.', 'daughter', 'are', 'you', 'my daughter', 'you are', 'daughter .', 'are my']\n",
      "= you are my daughter .\n",
      "< you are my daughter . <EOS>\n",
      "\n",
      "> ['they', 'armed', 're', '.', 'they re', 'armed .', 're armed']\n",
      "= they re armed .\n",
      "< they re armed . <EOS>\n",
      "\n",
      "> ['re', '.', 'no', 'different', 'you', 're no', 'you re', 'no different', 'different .']\n",
      "= you re no different .\n",
      "< you re no different . <EOS>\n",
      "\n",
      "> ['unhappy', '.', 'm', 'i', 'not', 'not unhappy', 'unhappy .', 'i m', 'm not']\n",
      "= i m not unhappy .\n",
      "< i m not well . <EOS>\n",
      "\n",
      "> ['fair', 'you', 're', '.', 'fair .', 'you re', 're fair']\n",
      "= you re fair .\n",
      "< you re fair . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'standing', 'we re', 'standing .', 're standing']\n",
      "= we re standing .\n",
      "< we re really . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'pretty', 're pretty', 'pretty .', 'you re']\n",
      "= you re pretty .\n",
      "< you re pretty . <EOS>\n",
      "\n",
      "> ['.', 'we', 'family', 'a', 'are', 'are a', 'family .', 'a family', 'we are']\n",
      "= we are a family .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< we are a family . <EOS>\n",
      "\n",
      "> ['happy', 'more', 'than', '.', 'm', 'i', 'more than', 'm more', 'happy .', 'i m', 'than happy']\n",
      "= i m more than happy .\n",
      "< i m more than happy . <EOS>\n",
      "\n",
      "> ['are', 'you', 'early', '.', 'early .', 'you are', 'are early']\n",
      "= you are early .\n",
      "< you are early . <EOS>\n",
      "\n",
      "> ['.', 'm', 'racist', 'i', 'not', 'not racist', 'i m', 'm not', 'racist .']\n",
      "= i m not racist .\n",
      "< i m not late . <EOS>\n",
      "\n",
      "> ['resourceful', '.', 'i', 'm', 'resourceful .', 'i m', 'm resourceful']\n",
      "= i m resourceful .\n",
      "< i m correct . <EOS>\n",
      "\n",
      "> ['stubborn', 're', '.', 'we', 'we re', 're stubborn', 'stubborn .']\n",
      "= we re stubborn .\n",
      "< we re stubborn . <EOS>\n",
      "\n",
      "> ['re', 'stylish', 'you', '.', 'very', 'stylish .', 'very stylish', 'you re', 're very']\n",
      "= you re very stylish .\n",
      "< you re very stylish . <EOS>\n",
      "\n",
      "> ['anymore', '.', 'm', 'i', 'married', 'not', 'anymore .', 'i m', 'not married', 'married anymore', 'm not']\n",
      "= i m not married anymore .\n",
      "< i m not married anymore . <EOS>\n",
      "\n",
      "> ['re', 'guy', '.', 'wonderful', 'a', 'you', 're a', 'you re', 'guy .', 'wonderful guy', 'a wonderful']\n",
      "= you re a wonderful guy .\n",
      "< you re a wonderful guy . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'resilient', 're resilient', 'we re', 'resilient .']\n",
      "= we re resilient .\n",
      "< we re resilient . <EOS>\n",
      "\n",
      "> ['re', 'the', 'shower', '.', 'they', 'in', 'the shower', 'in the', 'they re', 're in', 'shower .']\n",
      "= they re in the shower .\n",
      "< they re in the shower . <EOS>\n",
      "\n",
      "> ['he', '.', 's', 'a', 'bigot', 'a bigot', 'he s', 'bigot .', 's a']\n",
      "= he s a bigot .\n",
      "< he s a bigot . <EOS>\n",
      "\n",
      "> ['he', '.', 's', 'father', 'your', 'your father', 'father .', 's your', 'he s']\n",
      "= he s your father .\n",
      "< he s your father . <EOS>\n",
      "\n",
      "> ['documentary', 'making', '.', 'm', 'i', 'a', 'a documentary', 'i m', 'm making', 'making a', 'documentary .']\n",
      "= i m making a documentary .\n",
      "< i m a vegetarian . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'sleepy', 'sleepy .', 'you re', 're sleepy']\n",
      "= you re sleepy .\n",
      "< you re sleepy . <EOS>\n",
      "\n",
      "> ['he', 'friend', 'my', '.', 's', 'my friend', 'friend .', 'he s', 's my']\n",
      "= he s my friend .\n",
      "< he s my friend . <EOS>\n",
      "\n",
      "> ['same', 'all', 'the', '.', 'they', 'are', 'same .', 'the same', 'all the', 'are all', 'they are']\n",
      "= they are all the same .\n",
      "< they are the all . <EOS>\n",
      "\n",
      "> ['re', '.', 'fat', 'you', 'not', 'you re', 'fat .', 're not', 'not fat']\n",
      "= you re not fat .\n",
      "< you re not fat . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'prisoners', 're prisoners', 'we re', 'prisoners .']\n",
      "= we re prisoners .\n",
      "< we re prisoners . <EOS>\n",
      "\n",
      "> ['from', '.', 'i', 'am', 'portugal', 'am from', 'from portugal', 'portugal .', 'i am']\n",
      "= i am from portugal .\n",
      "< i am from . <EOS>\n",
      "\n",
      "> ['.', 'frantic', 'i', 'm', 'frantic .', 'i m', 'm frantic']\n",
      "= i m frantic .\n",
      "< i m frantic . <EOS>\n",
      "\n",
      "> ['he', '.', 'is', 'young', 'very', 'very young', 'is very', 'young .', 'he is']\n",
      "= he is very young .\n",
      "< he is very very daughter . <EOS>\n",
      "\n",
      "> ['he', 'his', 'parents', '.', 's', 'with', 'with his', 's with', 'his parents', 'he s', 'parents .']\n",
      "= he s with his parents .\n",
      "< he s open his . <EOS>\n",
      "\n",
      "> ['.', 'm', 'i', 'very', 'fortunate', 'm very', 'fortunate .', 'i m', 'very fortunate']\n",
      "= i m very fortunate .\n",
      "< i m very fortunate . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'winning', 're winning', 'you re', 'winning .']\n",
      "= you re winning .\n",
      "< you re winning . <EOS>\n",
      "\n",
      "> ['happy', '.', 'i', 'am', 'not', 'happy .', 'i am', 'not happy', 'am not']\n",
      "= i am not happy .\n",
      "< i am not happy . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'boring', 'you re', 're boring', 'boring .']\n",
      "= you re boring .\n",
      "< you re boring . <EOS>\n",
      "\n",
      "> ['.', 'i', 'm', 'humming', 'humming .', 'm humming', 'i m']\n",
      "= i m humming .\n",
      "< i m patient . <EOS>\n",
      "\n",
      "> ['he', 'fool', '.', 'no', 'is', 'no fool', 'he is', 'is no', 'fool .']\n",
      "= he is no fool .\n",
      "< he is no fool . <EOS>\n",
      "\n",
      "> ['re', '.', 'a', 'traitor', 'you', 're a', 'you re', 'traitor .', 'a traitor']\n",
      "= you re a traitor .\n",
      "< you re a traitor . <EOS>\n",
      "\n",
      "> ['avenue', '.', 'they', 'at', 'are', 'broadway', 'at broadway', 'are at', 'avenue .', 'they are', 'broadway avenue']\n",
      "= they are at broadway avenue .\n",
      "< they are approaching at right . <EOS>\n",
      "\n",
      "> ['childish', '.', 'sometimes', 'so', 'are', 'you', 'are so', 'sometimes .', 'so childish', 'you are', 'childish sometimes']\n",
      "= you are so childish sometimes .\n",
      "< you are so childish sometimes . <EOS>\n",
      "\n",
      "> ['!', 're', 'trapped', 'we', 'we re', 'trapped !', 're trapped']\n",
      "= we re trapped !\n",
      "< we re trapped ! <EOS>\n",
      "\n",
      "> ['incredible', 'you', 're', '.', 'incredible .', 'you re', 're incredible']\n",
      "= you re incredible .\n",
      "< you re incredible . <EOS>\n",
      "\n",
      "> ['our', '.', 'we', 'are', 'changing', 'clothes', 'we are', 'our clothes', 'changing our', 'are changing', 'clothes .']\n",
      "= we are changing our clothes .\n",
      "< we are the better . <EOS>\n",
      "\n",
      "> ['re', 'embarrassing', '.', 'yourself', 'you', 'yourself .', 'you re', 'embarrassing yourself', 're embarrassing']\n",
      "= you re embarrassing yourself .\n",
      "< you re completely . <EOS>\n",
      "\n",
      "> ['am', 'here', '.', 'i', 'a', 'stranger', 'here .', 'i am', 'stranger here', 'a stranger', 'am a']\n",
      "= i am a stranger here .\n",
      "< i am a stranger here . <EOS>\n",
      "\n",
      "> ['.', 'i', 'm', 'sorry', 'm sorry', 'i m', 'sorry .']\n",
      "= i m sorry .\n",
      "< i m sorry . <EOS>\n",
      "\n",
      "> ['s', 'he', '.', 'faking', 's faking', 'he s', 'faking .']\n",
      "= he s faking .\n",
      "< he s married . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'understanding', 'very', 'very understanding', 'you re', 're very', 'understanding .']\n",
      "= you re very understanding .\n",
      "< you re very understanding . <EOS>\n",
      "\n",
      "> ['re', 'stylish', 'you', '.', 'very', 'stylish .', 'very stylish', 'you re', 're very']\n",
      "= you re very stylish .\n",
      "< you re very stylish . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'fortunate', 'you re', 'fortunate .', 're fortunate']\n",
      "= you re fortunate .\n",
      "< you re fortunate . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'sleepy', 'sleepy .', 'you re', 're sleepy']\n",
      "= you re sleepy .\n",
      "< you re sleepy . <EOS>\n",
      "\n",
      "> ['strict', '.', 'i', 'm', 'm strict', 'i m', 'strict .']\n",
      "= i m strict .\n",
      "< i m strict . <EOS>\n",
      "\n",
      "> ['completely', 're', 'lost', '.', 'we', 'we re', 'completely lost', 'lost .', 're completely']\n",
      "= we re completely lost .\n",
      "< we re completely lost . <EOS>\n",
      "\n",
      "> ['he', 'stupid', '.', 's', 'not', 'not stupid', 'stupid .', 'he s', 's not']\n",
      "= he s not stupid .\n",
      "< he s not stupid . <EOS>\n",
      "\n",
      "> ['they', 'downstairs', 're', '.', 'they re', 'downstairs .', 're downstairs']\n",
      "= they re downstairs .\n",
      "< they re downstairs . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'very', 'brave', 'very brave', 'you re', 'brave .', 're very']\n",
      "= you re very brave .\n",
      "< you re very brave . <EOS>\n",
      "\n",
      "> ['he', 'absolute', 'fool', 'an', '.', 's', 'an absolute', 'fool .', 's an', 'he s', 'absolute fool']\n",
      "= he s an absolute fool .\n",
      "< he s so so . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'famous', 're famous', 'you re', 'famous .']\n",
      "= you re famous .\n",
      "< you re famous . <EOS>\n",
      "\n",
      "> ['re', '.', 'a', 'traitor', 'you', 're a', 'you re', 'traitor .', 'a traitor']\n",
      "= you re a traitor .\n",
      "< you re a traitor . <EOS>\n",
      "\n",
      "> ['!', 'ready', 'i', 'm', 'm ready', 'i m', 'ready !']\n",
      "= i m ready !\n",
      "< i m ready . <EOS>\n",
      "\n",
      "> ['.', 'patient', 'i', 'm', 'i m', 'm patient', 'patient .']\n",
      "= i m patient .\n",
      "< i m patient . <EOS>\n",
      "\n",
      "> ['re', 'you', 'wise', '.', 'very', 'you re', 'wise .', 're very', 'very wise']\n",
      "= you re very wise .\n",
      "< you re very wise . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'very', 'astute', 'astute .', 'you re', 're very', 'very astute']\n",
      "= you re very astute .\n",
      "< you re very astute . <EOS>\n",
      "\n",
      "> ['fashionable', 'you', 're', '.', 're fashionable', 'you re', 'fashionable .']\n",
      "= you re fashionable .\n",
      "< you re fashionable . <EOS>\n",
      "\n",
      "> ['around', '.', 'm', 'i', 'still', 'shopping', 'shopping around', 'still shopping', 'around .', 'i m', 'm still']\n",
      "= i m still shopping around .\n",
      "< i m still around . <EOS>\n",
      "\n",
      "> ['.', 'm', 'i', 'a', 'teacher', 'not', 'a teacher', 'not a', 'i m', 'teacher .', 'm not']\n",
      "= i m not a teacher .\n",
      "< i m not a teacher . <EOS>\n",
      "\n",
      "> ['re', 'booze', 'you', '.', 'out', 'of', 'booze .', 'you re', 'out of', 'of booze', 're out']\n",
      "= you re out of booze .\n",
      "< you re out of booze . <EOS>\n",
      "\n",
      "> ['hungry', '.', 'i', 'm', 'hungry .', 'i m', 'm hungry']\n",
      "= i m hungry .\n",
      "< i m hungry . <EOS>\n",
      "\n",
      "> ['.', 'almost', 'i', 'am', 'ready', 'am almost', 'almost ready', 'ready .', 'i am']\n",
      "= i am almost ready .\n",
      "< i am almost ready . <EOS>\n",
      "\n",
      "> ['certain', '.', 'i', 'm', 'certain .', 'm certain', 'i m']\n",
      "= i m certain .\n",
      "< i m certain . <EOS>\n",
      "\n",
      "> ['.', 'tired', 'i', 'm', 'tired .', 'i m', 'm tired']\n",
      "= i m tired .\n",
      "< i m tired . <EOS>\n",
      "\n",
      "> ['re', 'all', '.', 'we', 'bored', 'we re', 're all', 'all bored', 'bored .']\n",
      "= we re all bored .\n",
      "< we re all bored . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'separated', 're separated', 'we re', 'separated .']\n",
      "= we re separated .\n",
      "< we re separated . <EOS>\n",
      "\n",
      "> ['he', 'scientist', '.', 'is', 'a', 'a scientist', 'scientist .', 'he is', 'is a']\n",
      "= he is a scientist .\n",
      "< he is a scientist . <EOS>\n",
      "\n",
      "> ['he', '.', 'an', 'is', 'acrobat', 'an acrobat', 'he is', 'acrobat .', 'is an']\n",
      "= he is an acrobat .\n",
      "< he is an a girl . <EOS>\n",
      "\n",
      "> ['from', '.', 'm', 'i', 'tokyo', 'm from', 'from tokyo', 'i m', 'tokyo .']\n",
      "= i m from tokyo .\n",
      "< i m from tokyo . <EOS>\n",
      "\n",
      "> ['he', '.', 'is', 'a', 'daredevil', 'a daredevil', 'he is', 'is a', 'daredevil .']\n",
      "= he is a daredevil .\n",
      "< he is a married . <EOS>\n",
      "\n",
      "> ['re', '.', 'no', 'doctor', 'you', 'doctor .', 're no', 'you re', 'no doctor']\n",
      "= you re no doctor .\n",
      "< you re no doctor . <EOS>\n",
      "\n",
      "> ['re', 'impatient', '.', 'so', 'you', 'so impatient', 'impatient .', 'you re', 're so']\n",
      "= you re so impatient .\n",
      "< you re so impatient . <EOS>\n",
      "\n",
      "> ['.', 'is', 'she', 'sweater', 'a', 'knitting', 'a sweater', 'is knitting', 'she is', 'knitting a', 'sweater .']\n",
      "= she is knitting a sweater .\n",
      "< she is knitting a sweater . <EOS>\n",
      "\n",
      "> ['re', 'you', 'wise', '.', 'very', 'you re', 'wise .', 're very', 'very wise']\n",
      "= you re very wise .\n",
      "< you re very wise . <EOS>\n",
      "\n",
      "> ['.', 'i', 'eighteen', 'm', 'eighteen .', 'i m', 'm eighteen']\n",
      "= i m eighteen .\n",
      "< i m hiding . <EOS>\n",
      "\n",
      "> ['resourceful', '.', 'i', 'm', 'resourceful .', 'i m', 'm resourceful']\n",
      "= i m resourceful .\n",
      "< i m correct . <EOS>\n",
      "\n",
      "> ['re', 'worse', 'than', '.', 'tom', 'you', 'tom .', 're worse', 'worse than', 'you re', 'than tom']\n",
      "= you re worse than tom .\n",
      "< you re worse than tom . <EOS>\n",
      "\n",
      "> ['.', 'naive', 'are', 'unbelievably', 'you', 'naive .', 'you are', 'are unbelievably', 'unbelievably naive']\n",
      "= you are unbelievably naive .\n",
      "< you are too naive . <EOS>\n",
      "\n",
      "> ['re', 'man', '.', 'funny', 'a', 'you', 're a', 'you re', 'a funny', 'funny man', 'man .']\n",
      "= you re a funny man .\n",
      "< you re a man man . <EOS>\n",
      "\n",
      "> ['study', 'to', '.', 'i', 'am', 'going', 'am going', 'i am', 'going to', 'study .', 'to study']\n",
      "= i am going to study .\n",
      "< going going going to you . <EOS>\n",
      "\n",
      "> ['.', 'student', 'a', 'are', 'you', 'are a', 'a student', 'student .', 'you are']\n",
      "= you are a student .\n",
      "< you are a student . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'wonderful', 'you re', 'wonderful .', 're wonderful']\n",
      "= you re wonderful .\n",
      "< you re wonderful . <EOS>\n",
      "\n",
      "> ['offended', '.', 'i', 'm', 'offended .', 'i m', 'm offended']\n",
      "= i m offended .\n",
      "< i m offended . <EOS>\n",
      "\n",
      "> ['re', 'now', '.', 'we', 'going', 'now .', 'we re', 'going now', 're going']\n",
      "= we re going now .\n",
      "< we re right now . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'bossy', 'you re', 'bossy .', 're bossy']\n",
      "= you re bossy .\n",
      "< you re bossy . <EOS>\n",
      "\n",
      "> ['he', 'to', '.', 's', 'help', 'offered', 's offered', 'help .', 'offered to', 'to help', 'he s']\n",
      "= he s offered to help .\n",
      "< he s already to . <EOS>\n",
      "\n",
      "> ['re', 'talking', '.', 'we', 'done', 'we re', 'talking .', 're done', 'done talking']\n",
      "= we re done talking .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< we re done talking . <EOS>\n",
      "\n",
      "> ['young', 'he', '.', 'is', 'is young', 'young .', 'he is']\n",
      "= he is young .\n",
      "< he is young . <EOS>\n",
      "\n",
      "> ['all', '.', 'we', 'eleven', 'are', 'in', 'eleven in', 'are eleven', 'all .', 'we are', 'in all']\n",
      "= we are eleven in all .\n",
      "< we are all men . <EOS>\n",
      "\n",
      "> ['re', 'weak', 'too', '.', 'we', 'too weak', 'we re', 're too', 'weak .']\n",
      "= we re too weak .\n",
      "< we re too weak . <EOS>\n",
      "\n",
      "> ['.', 'awfully', 'm', 'i', 'tired', 'awfully tired', 'm awfully', 'i m', 'tired .']\n",
      "= i m awfully tired .\n",
      "< i m awfully tired . <EOS>\n",
      "\n",
      "> ['.', 'daughters', 'are', 'you', 'her', 'daughters .', 'are her', 'you are', 'her daughters']\n",
      "= you are her daughters .\n",
      "< you are her her . <EOS>\n",
      "\n",
      "> ['re', 'now', '.', 'we', 'friends', 'now .', 'we re', 're friends', 'friends now']\n",
      "= we re friends now .\n",
      "< we re friends now . <EOS>\n",
      "\n",
      "> ['.', 'm', 'strong', 'i', 'not', 'strong .', 'i m', 'm not', 'not strong']\n",
      "= i m not strong .\n",
      "< i m not strong . <EOS>\n",
      "\n",
      "> ['friend', 'my', 'best', '.', 'are', 'you', 'you are', 'best friend', 'my best', 'friend .', 'are my']\n",
      "= you are my best friend .\n",
      "< you are my friend . <EOS>\n",
      "\n",
      "> ['now', '.', 'work', 'm', 'i', 'at', 'i m', 'work now', 'm at', 'now .', 'at work']\n",
      "= i m at work now .\n",
      "< i m unmarried now . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'ready', 'not', 'we re', 'ready .', 're not', 'not ready']\n",
      "= we re not ready .\n",
      "< we re not ready . <EOS>\n",
      "\n",
      "> ['they', 're', '.', 'different', 're different', 'they re', 'different .']\n",
      "= they re different .\n",
      "< they re different . <EOS>\n",
      "\n",
      "> ['speaks', 'english', '.', 'she', 'good', 'she speaks', 'english .', 'good english', 'speaks good']\n",
      "= she speaks good english .\n",
      "< they speaks english . <EOS>\n",
      "\n",
      "> ['the', 'water', '.', 'she', 's', 'heating', 's heating', 'heating the', 'water .', 'she s', 'the water']\n",
      "= she s heating the water .\n",
      "< she s a the . <EOS>\n",
      "\n",
      "> ['re', 'terrified', 'all', '.', 'they', 'they re', 're all', 'all terrified', 'terrified .']\n",
      "= they re all terrified .\n",
      "< they re all terrified . <EOS>\n",
      "\n",
      "> ['mother', '.', 'she', 's', 'a', 'single', 'a single', 's a', 'single mother', 'mother .', 'she s']\n",
      "= she s a single mother .\n",
      "< she s a a girl . <EOS>\n",
      "\n",
      "> ['from', '.', 'm', 'i', 'kyoto', 'm from', 'from kyoto', 'i m', 'kyoto .']\n",
      "= i m from kyoto .\n",
      "< i m from kyoto . <EOS>\n",
      "\n",
      "> ['productive', 'you', 're', '.', 'you re', 'productive .', 're productive']\n",
      "= you re productive .\n",
      "< you re productive . <EOS>\n",
      "\n",
      "> ['tidy', '.', 'i', 'm', 'm tidy', 'i m', 'tidy .']\n",
      "= i m tidy .\n",
      "< i m tidy . <EOS>\n",
      "\n",
      "> ['timid', 're', 'you', '.', 'very', 'you re', 'timid .', 'very timid', 're very']\n",
      "= you re very timid .\n",
      "< you re very timid . <EOS>\n",
      "\n",
      "> ['he', 'lover', 'cat', '.', 's', 'a', 'cat lover', 's a', 'a cat', 'lover .', 'he s']\n",
      "= he s a cat lover .\n",
      "< he s a nice . <EOS>\n",
      "\n",
      "> ['magician', '.', 'm', 'i', 'a', 'not', 'not a', 'magician .', 'a magician', 'i m', 'm not']\n",
      "= i m not a magician .\n",
      "< i m not a soldier . <EOS>\n",
      "\n",
      "> ['he', 'sure', 'success', '.', 'is', 'of', 'success .', 'he is', 'of success', 'is sure', 'sure of']\n",
      "= he is sure of success .\n",
      "< he is sure sure . <EOS>\n",
      "\n",
      "> ['now', 'feeling', '.', 'm', 'fine', 'i', 'fine now', 'i m', 'feeling fine', 'now .', 'm feeling']\n",
      "= i m feeling fine now .\n",
      "< i m feeling fine now . <EOS>\n",
      "\n",
      "> ['re', 'bad', '.', 'they', 'not', 'not bad', 'they re', 'bad .', 're not']\n",
      "= they re not bad .\n",
      "< they re not bad . <EOS>\n",
      "\n",
      "> ['re', '.', 'awesome', 'really', 'you', 'awesome .', 'you re', 're really', 'really awesome']\n",
      "= you re really awesome .\n",
      "< you re really awesome . <EOS>\n",
      "\n",
      "> ['re', '.', 'such', 'liar', 'a', 'you', 'you re', 'such a', 'a liar', 're such', 'liar .']\n",
      "= you re such a liar .\n",
      "< you re such a liar . <EOS>\n",
      "\n",
      "> ['used', 'to', 'it', '.', 'm', 'i', 'to it', 'i m', 'used to', 'it .', 'm used']\n",
      "= i m used to it .\n",
      "< i m used to it . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'direct', 'very', 'direct .', 'you re', 'very direct', 're very']\n",
      "= you re very direct .\n",
      "< you re very direct . <EOS>\n",
      "\n",
      "> ['back', 'it', 'giving', '.', 'm', 'i', 'it back', 'i m', 'giving it', 'm giving', 'back .']\n",
      "= i m giving it back .\n",
      "< i m giving it back . <EOS>\n",
      "\n",
      "> ['he', 'water', '.', 's', 'hot', 'in', 'in hot', 'water .', 's in', 'hot water', 'he s']\n",
      "= he s in hot water .\n",
      "< he s in hot water . <EOS>\n",
      "\n",
      "> ['undecided', '.', 'm', 'i', 'still', 'm still', 'i m', 'still undecided', 'undecided .']\n",
      "= i m still undecided .\n",
      "< i m still . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'ourselves', 'deluding', 'ourselves .', 'we re', 'deluding ourselves', 're deluding']\n",
      "= we re deluding ourselves .\n",
      "< we re deluding ourselves . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'stuck', 're stuck', 'you re', 'stuck .']\n",
      "= you re stuck .\n",
      "< you re stuck . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'nice', 'very', 'very nice', 'you re', 're very', 'nice .']\n",
      "= you re very nice .\n",
      "< you re very nice . <EOS>\n",
      "\n",
      "> ['available', '.', 'm', 'i', 'not', 'available .', 'not available', 'i m', 'm not']\n",
      "= i m not available .\n",
      "< i m not available . <EOS>\n",
      "\n",
      "> ['.', 'a', 'are', 'you', 'person', 'good', 'are a', 'good person', 'you are', 'a good', 'person .']\n",
      "= you are a good person .\n",
      "< you are a good person . <EOS>\n",
      "\n",
      "> ['re', '.', 'liar', 'a', 'you', 're a', 'you re', 'liar .', 'a liar']\n",
      "= you re a liar .\n",
      "< you re a liar . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'direct', 'very', 'direct .', 'you re', 'very direct', 're very']\n",
      "= you re very direct .\n",
      "< you re very direct . <EOS>\n",
      "\n",
      "> ['re', 'teacher', 'the', '.', 'you', 're the', 'teacher .', 'you re', 'the teacher']\n",
      "= you re the teacher .\n",
      "< you re the teacher . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'safe', 're safe', 'you re', 'safe .']\n",
      "= you re safe .\n",
      "< you re safe . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'bright', 're bright', 'you re', 'bright .']\n",
      "= you re bright .\n",
      "< you re bright . <EOS>\n",
      "\n",
      "> ['slowpoke', 'he', '.', 's', 'a', 'a slowpoke', 'he s', 'slowpoke .', 's a']\n",
      "= he s a slowpoke .\n",
      "< he s a slowpoke . <EOS>\n",
      "\n",
      "> ['neatly', 'he', 'always', '.', 'dressed', 'is', 'dressed .', 'always neatly', 'he is', 'neatly dressed', 'is always']\n",
      "= he is always neatly dressed .\n",
      "< he is always neatly dressed . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'young', 'not', 'young .', 'we re', 're not', 'not young']\n",
      "= we re not young .\n",
      "< we re not young . <EOS>\n",
      "\n",
      "> ['he', '.', 'behind', 's', 'right', 'you', 'right behind', 'you .', 'behind you', 's right', 'he s']\n",
      "= he s right behind you .\n",
      "< he s right behind you . <EOS>\n",
      "\n",
      "> ['resentful', '.', 'i', 'm', 'm resentful', 'i m', 'resentful .']\n",
      "= i m resentful .\n",
      "< i m the . <EOS>\n",
      "\n",
      "> ['.', 'she', 'at', 'smiled', 'baby', 'her', 'smiled at', 'at her', 'she smiled', 'baby .', 'her baby']\n",
      "= she smiled at her baby .\n",
      "< she at of of . <EOS>\n",
      "\n",
      "> ['tennis', '.', 'she', 's', 'at', 'good', 'at tennis', 'good at', 's good', 'she s', 'tennis .']\n",
      "= she s good at tennis .\n",
      "< she s strong at one . <EOS>\n",
      "\n",
      "> ['making', '.', 'is', 'she', 'dinner', 'dinner .', 'making dinner', 'she is', 'is making']\n",
      "= she is making dinner .\n",
      "< she is making dinner . <EOS>\n",
      "\n",
      "> ['contagious', '.', 'i', 'm', 'm contagious', 'i m', 'contagious .']\n",
      "= i m contagious .\n",
      "< i m contagious . <EOS>\n",
      "\n",
      "> ['re', 'all', '.', 'we', 'cowards', 'we re', 're all', 'cowards .', 'all cowards']\n",
      "= we re all cowards .\n",
      "< we re all cowards . <EOS>\n",
      "\n",
      "> ['tennis', 'poor', '.', 'i', 'am', 'at', 'i am', 'at tennis', 'poor at', 'am poor', 'tennis .']\n",
      "= i am poor at tennis .\n",
      "< i am his a . <EOS>\n",
      "\n",
      "> ['re', 'alone', 'anymore', '.', 'you', 'not', 're not', 'anymore .', 'you re', 'not alone', 'alone anymore']\n",
      "= you re not alone anymore .\n",
      "< you re not alone anymore . <EOS>\n",
      "\n",
      "> ['natured', '.', 'is', 'she', 'good', 'natured .', 'is good', 'she is', 'good natured']\n",
      "= she is good natured .\n",
      "< she is good natured . <EOS>\n",
      "\n",
      "> ['giving', '.', 'm', 'i', 'up', 'not', 'up .', 'not giving', 'giving up', 'i m', 'm not']\n",
      "= i m not giving up .\n",
      "< i m not giving very skeptical . <EOS>\n",
      "\n",
      "> ['skinny', 'you', 're', '.', 'you re', 're skinny', 'skinny .']\n",
      "= you re skinny .\n",
      "< you re skinny . <EOS>\n",
      "\n",
      "> ['.', 're', 'starved', 'we', 'we re', 'starved .', 're starved']\n",
      "= we re starved .\n",
      "< we re starved . <EOS>\n",
      "\n",
      "> ['.', 'i', 'm', 'forgetful', 'm forgetful', 'i m', 'forgetful .']\n",
      "= i m forgetful .\n",
      "< i m forgetful . <EOS>\n",
      "\n",
      "> ['he', 'the', 'shower', '.', 's', 'in', 'the shower', 'in the', 's in', 'shower .', 'he s']\n",
      "= he s in the shower .\n",
      "< he s in the house . <EOS>\n",
      "\n",
      "> ['an', '.', 'she', 's', 'independent', 'thinker', 'independent thinker', 'an independent', 'she s', 's an', 'thinker .']\n",
      "= she s an independent thinker .\n",
      "< she s an independent . <EOS>\n",
      "\n",
      "> ['tomorrow', 'he', 'leaving', '.', 'chicago', 'is', 'leaving chicago', 'chicago tomorrow', 'is leaving', 'tomorrow .', 'he is']\n",
      "= he is leaving chicago tomorrow .\n",
      "< he is leaving chicago tomorrow . <EOS>\n",
      "\n",
      "> ['people', 'busy', '.', 'we', 'are', 'are busy', 'people .', 'busy people', 'we are']\n",
      "= we are busy people .\n",
      "< we are busy busy . <EOS>\n",
      "\n",
      "> ['he', 'gardener', '.', 's', 'a', 'a gardener', 'gardener .', 'he s', 's a']\n",
      "= he s a gardener .\n",
      "< he s a up . <EOS>\n",
      "\n",
      "> ['.', 'm', 'behind', 'right', 'i', 'him', 'right behind', 'i m', 'm right', 'behind him', 'him .']\n",
      "= i m right behind him .\n",
      "< i m behind behind him . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'loaded', 'you re', 're loaded', 'loaded .']\n",
      "= you re loaded .\n",
      "< you re loaded . <EOS>\n",
      "\n",
      "> ['them', 'to', 'grateful', '.', 'i', 'am', 'am grateful', 'to them', 'i am', 'them .', 'grateful to']\n",
      "= i am grateful to them .\n",
      "< i am our crazy . <EOS>\n",
      "\n",
      "> ['re', 'too', '.', 'they', 'fat', 'they re', 're too', 'fat .', 'too fat']\n",
      "= they re too fat .\n",
      "< they re too fat . <EOS>\n",
      "\n",
      "> ['obedient', 're', '.', 'we', 'we re', 're obedient', 'obedient .']\n",
      "= we re obedient .\n",
      "< we re obedient . <EOS>\n",
      "\n",
      "> ['they', 're', '.', 'boring', 'they re', 're boring', 'boring .']\n",
      "= they re boring .\n",
      "< they re boring . <EOS>\n",
      "\n",
      "> ['re', 'emotional', 'you', '.', 'very', 'very emotional', 'emotional .', 'you re', 're very']\n",
      "= you re very emotional .\n",
      "< you re very emotional . <EOS>\n",
      "\n",
      "> ['them', 'hard', '.', 'on', 'is', 'she', 'hard on', 'them .', 'she is', 'is hard', 'on them']\n",
      "= she is hard on them .\n",
      "< she is on on . . <EOS>\n",
      "\n",
      "> ['.', 'i', 'greedy', 'm', 'm greedy', 'i m', 'greedy .']\n",
      "= i m greedy .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< i m greedy . <EOS>\n",
      "\n",
      "> ['re', 'here', '.', 'we', 'done', 'here .', 'we re', 'done here', 're done']\n",
      "= we re done here .\n",
      "< we re here here . <EOS>\n",
      "\n",
      "> ['he', '.', 'something', 'mumbling', 's', 's mumbling', 'mumbling something', 'something .', 'he s']\n",
      "= he s mumbling something .\n",
      "< he s from your . <EOS>\n",
      "\n",
      "> ['impressed', '.', 'm', 'i', 'very', 'm very', 'impressed .', 'i m', 'very impressed']\n",
      "= i m very impressed .\n",
      "< i m very impressed . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'tough', 're tough', 'you re', 'tough .']\n",
      "= you re tough .\n",
      "< you re tough . <EOS>\n",
      "\n",
      "> ['the', '.', 'we', 'owners', 'are', 'new', 'new owners', 'the new', 'owners .', 'are the', 'we are']\n",
      "= we are the new owners .\n",
      "< we are new dogs . <EOS>\n",
      "\n",
      "> ['.', 'he', 'asleep', 'is', 'is asleep', 'he is', 'asleep .']\n",
      "= he is asleep .\n",
      "< he is probably with . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'overworked', 'overworked .', 'you re', 're overworked']\n",
      "= you re overworked .\n",
      "< you re overworked . <EOS>\n",
      "\n",
      "> ['.', 's', 'he', 'adorable', 'he s', 's adorable', 'adorable .']\n",
      "= he s adorable .\n",
      "< he s adorable . <EOS>\n",
      "\n",
      "> ['re', 'for', '.', 'they', 'you', 'looking', 'for you', 'you .', 're looking', 'they re', 'looking for']\n",
      "= they re looking for you .\n",
      "< they re looking for you . <EOS>\n",
      "\n",
      "> ['.', 'is', 'she', 'skiing', 'at', 'good', 'is good', 'at skiing', 'skiing .', 'good at', 'she is']\n",
      "= she is good at skiing .\n",
      "< she is good good skiing . <EOS>\n",
      "\n",
      "> ['re', 'emotional', 'you', '.', 'very', 'very emotional', 'emotional .', 'you re', 're very']\n",
      "= you re very emotional .\n",
      "< you re very emotional . <EOS>\n",
      "\n",
      "> ['available', '.', 'm', 'i', 'not', 'available .', 'not available', 'i m', 'm not']\n",
      "= i m not available .\n",
      "< i m not available . <EOS>\n",
      "\n",
      "> ['side', '.', 'm', 'i', 'by', 'your', 'your side', 'by your', 'i m', 'm by', 'side .']\n",
      "= i m by your side .\n",
      "< i m by your side . <EOS>\n",
      "\n",
      "> ['he', '.', 's', 'a', 'jesuit', 'a jesuit', 'jesuit .', 'he s', 's a']\n",
      "= he s a jesuit .\n",
      "< he s a powerful . <EOS>\n",
      "\n",
      "> ['.', 'guilty', 'm', 'i', 'not', 'not guilty', 'i m', 'm not', 'guilty .']\n",
      "= i m not guilty .\n",
      "< i m not and . <EOS>\n",
      "\n",
      "> ['tough', '.', 'i', 'm', 'm tough', 'i m', 'tough .']\n",
      "= i m tough .\n",
      "< i m tough . <EOS>\n",
      "\n",
      "> ['it', '.', 'm', 'i', 'denying', 'not', 'not denying', 'denying it', 'i m', 'it .', 'm not']\n",
      "= i m not denying it .\n",
      "< i m not it . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'clever', 'you re', 're clever', 'clever .']\n",
      "= you re clever .\n",
      "< you re clever . <EOS>\n",
      "\n",
      "> ['super', '.', 'm', 'hungry', 'i', 'hungry .', 'i m', 'm super', 'super hungry']\n",
      "= i m super hungry .\n",
      "< i m hungry . <EOS>\n",
      "\n",
      "> ['he', '.', 's', 'self', 'employed', 's self', 'he s', 'self employed', 'employed .']\n",
      "= he s self employed .\n",
      "< he s away . <EOS>\n",
      "\n",
      "> ['.', 'm', 'bluffing', 'i', 'not', 'bluffing .', 'not bluffing', 'i m', 'm not']\n",
      "= i m not bluffing .\n",
      "< i m not bluffing . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'big', 'big .', 'you re', 're big']\n",
      "= you re big .\n",
      "< you re big . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'crafty', 're crafty', 'you re', 'crafty .']\n",
      "= you re crafty .\n",
      "< you re crafty . <EOS>\n",
      "\n",
      "> ['already', '.', 'm', 'i', 'done', 'done .', 'already done', 'i m', 'm already']\n",
      "= i m already done .\n",
      "< i m already done . <EOS>\n",
      "\n",
      "> ['re', 'talented', 'you', '.', 'very', 'you re', 'very talented', 're very', 'talented .']\n",
      "= you re very talented .\n",
      "< you re very talented . <EOS>\n",
      "\n",
      "> ['friend', '.', 'm', 'i', 'your', 'm your', 'friend .', 'i m', 'your friend']\n",
      "= i m your friend .\n",
      "< i m your friend . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'dieting', 'we re', 'dieting .', 're dieting']\n",
      "= we re dieting .\n",
      "< we re dieting . <EOS>\n",
      "\n",
      "> ['wrong', 'he', '.', 'probably', 's', 'probably wrong', 'wrong .', 'he s', 's probably']\n",
      "= he s probably wrong .\n",
      "< he s probably at go . <EOS>\n",
      "\n",
      "> ['sad', '.', 'i', 'm', 'sad .', 'i m', 'm sad']\n",
      "= i m sad .\n",
      "< i m sad . <EOS>\n",
      "\n",
      "> ['started', 'the', 'from', '.', 'she', 'summit', 'from the', 'the summit', 'she started', 'summit .', 'started from']\n",
      "= she started from the summit .\n",
      "< he too learning learning . <EOS>\n",
      "\n",
      "> ['my', '.', 'm', 'handkerchiefs', 'i', 'ironing', 'handkerchiefs .', 'i m', 'my handkerchiefs', 'ironing my', 'm ironing']\n",
      "= i m ironing my handkerchiefs .\n",
      "< i m reliable reliable . <EOS>\n",
      "\n",
      "> ['re', 'alone', 'all', '.', 'you', 're all', 'you re', 'all alone', 'alone .']\n",
      "= you re all alone .\n",
      "< you re all alone . <EOS>\n",
      "\n",
      "> ['happy', 're', '.', 'they', 'not', 'happy .', 'they re', 're not', 'not happy']\n",
      "= they re not happy .\n",
      "< they re not happy . <EOS>\n",
      "\n",
      "> ['disloyal', 'you', 're', '.', 'disloyal .', 'you re', 're disloyal']\n",
      "= you re disloyal .\n",
      "< you re disloyal . <EOS>\n",
      "\n",
      "> ['fair', 'you', 're', '.', 'fair .', 'you re', 're fair']\n",
      "= you re fair .\n",
      "< you re fair . <EOS>\n",
      "\n",
      "> ['re', '.', 'right', 'course', 'you', 'of', 'right of', 're right', 'you re', 'of course', 'course .']\n",
      "= you re right of course .\n",
      "< you re right of course . <EOS>\n",
      "\n",
      "> ['.', 'student', 'a', 'are', 'you', 'are a', 'a student', 'student .', 'you are']\n",
      "= you are a student .\n",
      "< you are a student . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'faithful', 're faithful', 'you re', 'faithful .']\n",
      "= you re faithful .\n",
      "< you re faithful . <EOS>\n",
      "\n",
      "> ['.', 'thrilled', 'i', 'm', 'thrilled .', 'i m', 'm thrilled']\n",
      "= i m thrilled .\n",
      "< i m thrilled . <EOS>\n",
      "\n",
      "> ['they', 're', 'tired', '.', 'they re', 're tired', 'tired .']\n",
      "= they re tired .\n",
      "< they re tired . <EOS>\n",
      "\n",
      "> ['.', 'innocent', 'i', 'm', 'innocent .', 'i m', 'm innocent']\n",
      "= i m innocent .\n",
      "< i m innocent . <EOS>\n",
      "\n",
      "> ['fond', '.', 'i', 'am', 'music', 'of', 'i am', 'fond of', 'of music', 'music .', 'am fond']\n",
      "= i am fond of music .\n",
      "< she of of of of . <EOS>\n",
      "\n",
      "> ['re', 'luck', '.', 'we', 'in', 'we re', 'luck .', 're in', 'in luck']\n",
      "= we re in luck .\n",
      "< we re in luck . <EOS>\n",
      "\n",
      "> ['kind', '.', 'they', 'are', 'very', 'kind .', 'are very', 'very kind', 'they are']\n",
      "= they are very kind .\n",
      "< they are very kind . <EOS>\n",
      "\n",
      "> ['is', 'kind', '.', 'she', 'is kind', 'kind .', 'she is']\n",
      "= she is kind .\n",
      "< she is kind . <EOS>\n",
      "\n",
      "> ['slow', 're', 'too', '.', 'you', 'you re', 're too', 'too slow', 'slow .']\n",
      "= you re too slow .\n",
      "< you re too slow . <EOS>\n",
      "\n",
      "> ['ambition', 'he', '.', 'is', 'full', 'of', 'ambition .', 'he is', 'full of', 'is full', 'of ambition']\n",
      "= he is full of ambition .\n",
      "< he is at of music . <EOS>\n",
      "\n",
      "> ['oldest', 'the', '.', 'm', 'i', 'the oldest', 'i m', 'm the', 'oldest .']\n",
      "= i m the oldest .\n",
      "< i m the oldest . <EOS>\n",
      "\n",
      "> ['he', '.', 's', 'so', 'cute', 's so', 'cute .', 'he s', 'so cute']\n",
      "= he s so cute .\n",
      "< he s so cute . <EOS>\n",
      "\n",
      "> ['re', 'the', 'best', '.', 'you', 're the', 'the best', 'you re', 'best .']\n",
      "= you re the best .\n",
      "< you re the best . <EOS>\n",
      "\n",
      "> ['re', '.', 'company', 'we', 'having', 'we re', 'company .', 'having company', 're having']\n",
      "= we re having company .\n",
      "< we re having . <EOS>\n",
      "\n",
      "> ['undressing', '.', 'i', 'am', 'am undressing', 'undressing .', 'i am']\n",
      "= i am undressing .\n",
      "< i am undressing . <EOS>\n",
      "\n",
      "> ['s', 'he', '.', 'swiss', 'swiss .', 's swiss', 'he s']\n",
      "= he s swiss .\n",
      "< he s swiss . <EOS>\n",
      "\n",
      "> ['last', 're', 'hope', '.', 'we', 'your', 'hope .', 're your', 'we re', 'last hope', 'your last']\n",
      "= we re your last hope .\n",
      "< we re your last back . <EOS>\n",
      "\n",
      "> ['the', 'from', '.', 'm', 'city', 'i', 'from the', 'the city', 'i m', 'm from', 'city .']\n",
      "= i m from the city .\n",
      "< i m from the city . <EOS>\n",
      "\n",
      "> ['re', 'just', '.', 'we', 'beginning', 'we re', 're just', 'beginning .', 'just beginning']\n",
      "= we re just beginning .\n",
      "< we re just . <EOS>\n",
      "\n",
      "> ['here', '.', 'i', 'am', 'new', 'here .', 'i am', 'new here', 'am new']\n",
      "= i am new here .\n",
      "< i am new here . <EOS>\n",
      "\n",
      "> ['.', 'photographer', 'm', 'i', 'a', 'm a', 'a photographer', 'i m', 'photographer .']\n",
      "= i m a photographer .\n",
      "< i m a jealous . <EOS>\n",
      "\n",
      "> ['.', 'he', 'poor', 'is', 'poor .', 'he is', 'is poor']\n",
      "= he is poor .\n",
      "< he is poor . <EOS>\n",
      "\n",
      "> ['happy', 're', '.', 'we', 'not', 'happy .', 'we re', 're not', 'not happy']\n",
      "= we re not happy .\n",
      "< we re not happy . <EOS>\n",
      "\n",
      "> ['now', '.', 'she', 'doctor', 's', 'a', 'a doctor', 's a', 'doctor now', 'she s', 'now .']\n",
      "= she s a doctor now .\n",
      "< she s a powerful . <EOS>\n",
      "\n",
      "> ['pretty', '.', 'she', 's', 'very', 'pretty .', 'very pretty', 's very', 'she s']\n",
      "= she s very pretty .\n",
      "< she s very pretty . <EOS>\n",
      "\n",
      "> ['re', 'forward', 'you', '.', 'very', 'very forward', 'you re', 're very', 'forward .']\n",
      "= you re very forward .\n",
      "< you re very forward . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'disgusting', 're disgusting', 'you re', 'disgusting .']\n",
      "= you re disgusting .\n",
      "< you re disgusting . <EOS>\n",
      "\n",
      "> ['everyone', 'he', '.', 'is', 'by', 'hated', 'everyone .', 'he is', 'by everyone', 'is hated', 'hated by']\n",
      "= he is hated by everyone .\n",
      "< he is is old me . <EOS>\n",
      "\n",
      "> ['unhappy', '.', 'm', 'i', 'not', 'not unhappy', 'unhappy .', 'i m', 'm not']\n",
      "= i m not unhappy .\n",
      "< i m not well . <EOS>\n",
      "\n",
      "> ['he', '.', 'is', 'good', 'at', 'soccer', 'soccer .', 'is good', 'at soccer', 'he is', 'good at']\n",
      "= he is good at soccer .\n",
      "< he is good at busy . <EOS>\n",
      "\n",
      "> ['re', 'annoying', '.', 'really', 'you', 'really annoying', 'you re', 're really', 'annoying .']\n",
      "= you re really annoying .\n",
      "< you re really annoying . <EOS>\n",
      "\n",
      "> ['dissatisfied', 'he', 'always', '.', 's', 'dissatisfied .', 's always', 'always dissatisfied', 'he s']\n",
      "= he s always dissatisfied .\n",
      "< he s always . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'grounded', 're grounded', 'you re', 'grounded .']\n",
      "= you re grounded .\n",
      "< you re grounded . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'clever', 'very', 'you re', 'clever .', 're very', 'very clever']\n",
      "= you re very clever .\n",
      "< you re very clever . <EOS>\n",
      "\n",
      "> ['re', 'you', 'tall', '.', 'very', 'tall .', 'you re', 'very tall', 're very']\n",
      "= you re very tall .\n",
      "< you re very tall . <EOS>\n",
      "\n",
      "> ['re', 'trusting', 'too', '.', 'you', 'trusting .', 'you re', 'too trusting', 're too']\n",
      "= you re too trusting .\n",
      "< you re too trusting . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'not', 'competitors', 'partners', 'partners .', 're competitors', 'we re', 'not partners', 'competitors not']\n",
      "= we re competitors not partners .\n",
      "< we re not . <EOS>\n",
      "\n",
      "> ['they', 're', '.', 'staying', 'they re', 'staying .', 're staying']\n",
      "= they re staying .\n",
      "< they re staying . <EOS>\n",
      "\n",
      "> ['re', 'the', '.', 'master', 'you', 're the', 'the master', 'you re', 'master .']\n",
      "= you re the master .\n",
      "< you re the master . <EOS>\n",
      "\n",
      "> ['finished', '.', 'i', 'm', 'i m', 'm finished', 'finished .']\n",
      "= i m finished .\n",
      "< i m finished . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'saved', 'we re', 're saved', 'saved .']\n",
      "= we re saved .\n",
      "< we re saved . <EOS>\n",
      "\n",
      "> ['.', 'is', 'she', 'doctor', 'a', 'doctor .', 'a doctor', 'is a', 'she is']\n",
      "= she is a doctor .\n",
      "< she is a doctor . <EOS>\n",
      "\n",
      "> ['re', '.', 'you', 'normal', 'not', 'normal .', 'you re', 're not', 'not normal']\n",
      "= you re not normal .\n",
      "< you re not normal . <EOS>\n",
      "\n",
      "> ['dancer', '.', 'is', 'she', 'a', 'good', 'is a', 'good dancer', 'she is', 'a good', 'dancer .']\n",
      "= she is a good dancer .\n",
      "< she is a good girl . <EOS>\n",
      "\n",
      "> ['re', 'you', 'efficient', '.', 'very', 'very efficient', 'you re', 're very', 'efficient .']\n",
      "= you re very efficient .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< you re very efficient . <EOS>\n",
      "\n",
      "> ['he', '.', 'angry', 'is', 'very', 'is very', 'very angry', 'he is', 'angry .']\n",
      "= he is very angry .\n",
      "< he is very angry . <EOS>\n",
      "\n",
      "> ['anymore', 'angry', 'm', '.', 'i', 'not', 'anymore .', 'i m', 'not angry', 'angry anymore', 'm not']\n",
      "= i m not angry anymore .\n",
      "< i m not serious . <EOS>\n",
      "\n",
      "> ['he', '.', 's', 'boy', 'a', 'good', 's a', 'good boy', 'boy .', 'a good', 'he s']\n",
      "= he s a good boy .\n",
      "< he s a good . <EOS>\n",
      "\n",
      "> ['cold', '.', 'm', 'i', 'very', 'm very', 'cold .', 'i m', 'very cold']\n",
      "= i m very cold .\n",
      "< i m very cold . <EOS>\n",
      "\n",
      "> ['happy', '.', 'perfectly', 'm', 'i', 'm perfectly', 'perfectly happy', 'i m', 'happy .']\n",
      "= i m perfectly happy .\n",
      "< i m perfectly happy . <EOS>\n",
      "\n",
      "> ['s', '.', 'pregnant', 'she', 's pregnant', 'pregnant .', 'she s']\n",
      "= she s pregnant .\n",
      "< she s pregnant . <EOS>\n",
      "\n",
      "> ['he', '.', 'nasty', 'is', 'nasty .', 'he is', 'is nasty']\n",
      "= he is nasty .\n",
      "< he is wearing . <EOS>\n",
      "\n",
      "> ['.', 'tourist', 'i', 'a', 'am', 'a tourist', 'am a', 'tourist .', 'i am']\n",
      "= i am a tourist .\n",
      "< i am a cat . <EOS>\n",
      "\n",
      "> ['!', 'sleepy', 'i', 'm', 'sleepy !', 'i m', 'm sleepy']\n",
      "= i m sleepy !\n",
      "< i m sleepy ! <EOS>\n",
      "\n",
      "> ['.', 'i', 'sure', 'am', 'am sure', 'i am', 'sure .']\n",
      "= i am sure .\n",
      "< i am sure . <EOS>\n",
      "\n",
      "> ['happy', '.', 'm', 'i', 'quite', 'happy .', 'm quite', 'i m', 'quite happy']\n",
      "= i m quite happy .\n",
      "< i m quite happy . <EOS>\n",
      "\n",
      "> ['fashionable', 'you', 're', '.', 're fashionable', 'you re', 'fashionable .']\n",
      "= you re fashionable .\n",
      "< you re fashionable . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'going', 'shopping', 'shopping .', 'we re', 'going shopping', 're going']\n",
      "= we re going shopping .\n",
      "< we re going shopping . <EOS>\n",
      "\n",
      "> ['been', 'poisoned', '.', 'she', 's', 's been', 'she s', 'been poisoned', 'poisoned .']\n",
      "= she s been poisoned .\n",
      "< she s always . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'perceptive', 'very', 'very perceptive', 'you re', 're very', 'perceptive .']\n",
      "= you re very perceptive .\n",
      "< you re very perceptive . <EOS>\n",
      "\n",
      "> ['beautiful', '.', 'she', 's', 'very', 'beautiful .', 'very beautiful', 's very', 'she s']\n",
      "= she s very beautiful .\n",
      "< she s very beautiful . <EOS>\n",
      "\n",
      "> ['he', 'actor', '.', 'an', 'is', 'he is', 'an actor', 'is an', 'actor .']\n",
      "= he is an actor .\n",
      "< he is an japanese japanese . <EOS>\n",
      "\n",
      "> ['dreaming', 'he', 'always', '.', 'is', 'day', 'day dreaming', 'always day', 'he is', 'dreaming .', 'is always']\n",
      "= he is always day dreaming .\n",
      "< he is always capable . <EOS>\n",
      "\n",
      "> ['shaken', '.', 'i', 'm', 'shaken .', 'm shaken', 'i m']\n",
      "= i m shaken .\n",
      "< i m shaken . <EOS>\n",
      "\n",
      "> ['re', '.', 'you', 'dead', 'not', 'dead .', 'you re', 'not dead', 're not']\n",
      "= you re not dead .\n",
      "< you re not dead . <EOS>\n",
      "\n",
      "> ['he', 'friend', 'my', 'best', '.', 's', 'he s', 's my', 'best friend', 'my best', 'friend .']\n",
      "= he s my best friend .\n",
      "< he s my best friend . <EOS>\n",
      "\n",
      "> ['re', 'doing', 'it', '.', 'right', 'you', 'right .', 'you re', 'doing it', 'it right', 're doing']\n",
      "= you re doing it right .\n",
      "< you re doing it right . <EOS>\n",
      "\n",
      "> ['re', '.', 'dressed', 'we', 'not', 'we re', 'dressed .', 're not', 'not dressed']\n",
      "= we re not dressed .\n",
      "< we re not dressed . <EOS>\n",
      "\n",
      "> ['he', '.', 'lazy', 'is', 'is lazy', 'he is', 'lazy .']\n",
      "= he is lazy .\n",
      "< he is lazy . <EOS>\n",
      "\n",
      "> ['him', '.', 'shot', 'she', 'shot him', 'she shot', 'him .']\n",
      "= she shot him .\n",
      "< she shot him . <EOS>\n",
      "\n",
      "> ['re', 'pathetic', '.', 'so', 'you', 'pathetic .', 'so pathetic', 'you re', 're so']\n",
      "= you re so pathetic .\n",
      "< you re so pathetic . <EOS>\n",
      "\n",
      "> ['he', '.', 'is', 'a', 'physicist', 'a physicist', 'physicist .', 'he is', 'is a']\n",
      "= he is a physicist .\n",
      "< he is a physicist . <EOS>\n",
      "\n",
      "> ['re', 'invited', '.', 'we', 'not', 'not invited', 'we re', 'invited .', 're not']\n",
      "= we re not invited .\n",
      "< we re not invited . <EOS>\n",
      "\n",
      "> ['re', 'you', 'wise', '.', 'very', 'you re', 'wise .', 're very', 'very wise']\n",
      "= you re very wise .\n",
      "< you re very wise . <EOS>\n",
      "\n",
      "> ['re', 'fit', '.', 'you', 'not', 'you re', 're not', 'fit .', 'not fit']\n",
      "= you re not fit .\n",
      "< you re not fit . <EOS>\n",
      "\n",
      "> ['invited', 'aren', '.', 'you', 't', 'you aren', 'aren t', 'invited .', 't invited']\n",
      "= you aren t invited .\n",
      "< you aren t invited . <EOS>\n",
      "\n",
      "> ['he', '.', 'old', 'is', 'is old', 'he is', 'old .']\n",
      "= he is old .\n",
      "< he is old . <EOS>\n",
      "\n",
      "> ['re', 'now', '.', 'we', 'ready', 're ready', 'we re', 'ready now', 'now .']\n",
      "= we re ready now .\n",
      "< we re ready now . <EOS>\n",
      "\n",
      "> ['re', 'you', 'sophisticated', '.', 'very', 'sophisticated .', 'you re', 'very sophisticated', 're very']\n",
      "= you re very sophisticated .\n",
      "< you re very sophisticated . <EOS>\n",
      "\n",
      "> ['he', 'it', '.', 's', 'raking', 'in', 'in .', 'it in', 'raking it', 's raking', 'he s']\n",
      "= he s raking it in .\n",
      "< he s raking it in . <EOS>\n",
      "\n",
      "> ['re', 'all', 'here', '.', 'they', 'all here', 'they re', 're all', 'here .']\n",
      "= they re all here .\n",
      "< they re all here . <EOS>\n",
      "\n",
      "> ['re', 'you', '.', 'attractive', 'very', 'attractive .', 'very attractive', 'you re', 're very']\n",
      "= you re very attractive .\n",
      "< you re very attractive . <EOS>\n",
      "\n",
      "> ['.', 'little', 'm', 'disappointed', 'i', 'a', 'm a', 'i m', 'disappointed .', 'little disappointed', 'a little']\n",
      "= i m a little disappointed .\n",
      "< i m a little disappointed . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'powerful', 're powerful', 'we re', 'powerful .']\n",
      "= we re powerful .\n",
      "< we re powerful . <EOS>\n",
      "\n",
      "> ['professor', '.', 'i', 'a', 'am', 'a professor', 'am a', 'professor .', 'i am']\n",
      "= i am a professor .\n",
      "< i am a of reading . <EOS>\n",
      "\n",
      "> ['re', 'stupid', '.', 'unbelievably', 'you', 're unbelievably', 'stupid .', 'you re', 'unbelievably stupid']\n",
      "= you re unbelievably stupid .\n",
      "< you re unbelievably stupid . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'surrounded', 'you re', 're surrounded', 'surrounded .']\n",
      "= you re surrounded .\n",
      "< you re surrounded . <EOS>\n",
      "\n",
      "> ['.', 'i', 'm', 'retired', 'm retired', 'i m', 'retired .']\n",
      "= i m retired .\n",
      "< i m retired . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'callous', 're callous', 'callous .', 'you re']\n",
      "= you re callous .\n",
      "< you re callous . <EOS>\n",
      "\n",
      "> ['sandwich', '.', 'eating', 'm', 'i', 'a', 'eating a', 'm eating', 'a sandwich', 'i m', 'sandwich .']\n",
      "= i m eating a sandwich .\n",
      "< i m eating woman . <EOS>\n",
      "\n",
      "> ['conscientious', 're', '.', 'we', 'we re', 'conscientious .', 're conscientious']\n",
      "= we re conscientious .\n",
      "< we re conscientious . <EOS>\n",
      "\n",
      "> ['.', 'm', 'discouraged', 'i', 'not', 'not discouraged', 'i m', 'm not', 'discouraged .']\n",
      "= i m not discouraged .\n",
      "< i m not discouraged . <EOS>\n",
      "\n",
      "> ['observant', '.', 'i', 'm', 'observant .', 'i m', 'm observant']\n",
      "= i m observant .\n",
      "< i m observant . <EOS>\n",
      "\n",
      "> ['mentally', 'he', '.', 'is', 'handicapped', 'is mentally', 'mentally handicapped', 'he is', 'handicapped .']\n",
      "= he is mentally handicapped .\n",
      "< he is kind . <EOS>\n",
      "\n",
      "> ['they', 'downstairs', 're', '.', 'they re', 'downstairs .', 're downstairs']\n",
      "= they re downstairs .\n",
      "< they re downstairs . <EOS>\n",
      "\n",
      "> ['re', 'sync', '.', 'we', 'in', 'we re', 'in sync', 'sync .', 're in']\n",
      "= we re in sync .\n",
      "< we re in sync . <EOS>\n",
      "\n",
      "> ['patient', 'being', '.', 'm', 'i', 'being patient', 'i m', 'm being', 'patient .']\n",
      "= i m being patient .\n",
      "< i m being . <EOS>\n",
      "\n",
      "> ['busy', 'too', '.', 'm', 'i', 'too .', 'm busy', 'i m', 'busy too']\n",
      "= i m busy too .\n",
      "< i m busy too . <EOS>\n",
      "\n",
      "> ['re', '.', 'you', 'normal', 'not', 'normal .', 'you re', 're not', 'not normal']\n",
      "= you re not normal .\n",
      "< you re not normal . <EOS>\n",
      "\n",
      "> ['.', 'involved', 'i', 'm', 'involved .', 'i m', 'm involved']\n",
      "= i m involved .\n",
      "< i m involved . <EOS>\n",
      "\n",
      "> ['are', 'naughty', 'you', '.', 'naughty .', 'you are', 'are naughty']\n",
      "= you are naughty .\n",
      "< you are naughty . <EOS>\n",
      "\n",
      "> ['.', 'i', 'm', 'retired', 'm retired', 'i m', 'retired .']\n",
      "= i m retired .\n",
      "< i m retired . <EOS>\n",
      "\n",
      "> ['childish', '.', 'sometimes', 'so', 'are', 'you', 'are so', 'sometimes .', 'so childish', 'you are', 'childish sometimes']\n",
      "= you are so childish sometimes .\n",
      "< you are so childish sometimes . <EOS>\n",
      "\n",
      "> ['re', 'me', 'all', '.', 'you', 'against', 'me .', 'all against', 'against me', 're all', 'you re']\n",
      "= you re all against me .\n",
      "< you re all against me . <EOS>\n",
      "\n",
      "> ['.', 'you', 're', 'big', 'big .', 'you re', 're big']\n",
      "= you re big .\n",
      "< you re big . <EOS>\n",
      "\n",
      "> ['love', 'my', '.', 'she', 's', 'first', 's my', 'love .', 'first love', 'she s', 'my first']\n",
      "= she s my first love .\n",
      "< she s my better my better . <EOS>\n",
      "\n",
      "> ['low', '.', 'feeling', 'm', 'i', 'low .', 'm feeling', 'i m', 'feeling low']\n",
      "= i m feeling low .\n",
      "< i m feeling low . <EOS>\n",
      "\n",
      "> ['.', 'm', 'i', 'a', 'teacher', 'm a', 'teacher .', 'i m', 'a teacher']\n",
      "= i m a teacher .\n",
      "< i m a teacher . <EOS>\n",
      "\n",
      "> ['re', 'being', '.', 'malicious', 'you', 'you re', 'being malicious', 'malicious .', 're being']\n",
      "= you re being malicious .\n",
      "< you re being . <EOS>\n",
      "\n",
      "> ['addicted', '.', 'i', 'm', 'm addicted', 'i m', 'addicted .']\n",
      "= i m addicted .\n",
      "< i m addicted . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'north', 'going', 'going north', 'we re', 'north .', 're going']\n",
      "= we re going north .\n",
      "< we re going hungry . <EOS>\n",
      "\n",
      "> ['he', '.', 'is', 'big', 'a', 'prankster', 'a big', 'is a', 'big prankster', 'prankster .', 'he is']\n",
      "= he is a big prankster .\n",
      "< he is a big . <EOS>\n",
      "\n",
      "> ['just', '.', 'm', 'i', 'tired', 'm just', 'just tired', 'i m', 'tired .']\n",
      "= i m just tired .\n",
      "< i m just tired . <EOS>\n",
      "\n",
      "> ['college', 'to', '.', 'm', 'i', 'going', 'college .', 'to college', 'i m', 'm going', 'going to']\n",
      "= i m going to college .\n",
      "< i m going to . <EOS>\n",
      "\n",
      "> ['cruel', 'you', 're', '.', 'cruel .', 'you re', 're cruel']\n",
      "= you re cruel .\n",
      "< you re cruel . <EOS>\n",
      "\n",
      "> ['re', 'oldest', 'the', '.', 'you', 're the', 'you re', 'the oldest', 'oldest .']\n",
      "= you re the oldest .\n",
      "< you re the oldest . <EOS>\n",
      "\n",
      "> ['you', 're', '.', 'horrible', 'horrible .', 'you re', 're horrible']\n",
      "= you re horrible .\n",
      "< you re horrible . <EOS>\n",
      "\n",
      "> ['he', '.', 's', 'a', 'bigot', 'a bigot', 'he s', 'bigot .', 's a']\n",
      "= he s a bigot .\n",
      "< he s a bigot . <EOS>\n",
      "\n",
      "> ['trouble', 'now', '.', 'm', 'i', 'in', 'i m', 'in trouble', 'm in', 'now .', 'trouble now']\n",
      "= i m in trouble now .\n",
      "< i m in trouble . <EOS>\n",
      "\n",
      "> ['re', '.', 'sweet', 'so', 'you', 'sweet .', 'so sweet', 'you re', 're so']\n",
      "= you re so sweet .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< you re so sweet . <EOS>\n",
      "\n",
      "> ['.', 'ready', 'i', 'm', 'm ready', 'ready .', 'i m']\n",
      "= i m ready .\n",
      "< i m ready . <EOS>\n",
      "\n",
      "> ['he', '.', 'is', 'energy', 'full', 'of', 'of energy', 'he is', 'full of', 'is full', 'energy .']\n",
      "= he is full of energy .\n",
      "< he is bad bad but . <EOS>\n",
      "\n",
      "> ['re', 'totally', '.', 'ignorant', 'you', 'ignorant .', 'totally ignorant', 'you re', 're totally']\n",
      "= you re totally ignorant .\n",
      "< you re totally ignorant . <EOS>\n",
      "\n",
      "> ['alone', '.', 'we', 'are', 'not', 'are not', 'not alone', 'alone .', 'we are']\n",
      "= we are not alone .\n",
      "< we are not alone . <EOS>\n",
      "\n",
      "> ['.', 'i', 'm', 'retired', 'm retired', 'i m', 'retired .']\n",
      "= i m retired .\n",
      "< i m retired . <EOS>\n",
      "\n",
      "> ['.', 'm', 'cracking', 'i', 'up', 'cracking up', 'm cracking', 'i m', 'up .']\n",
      "= i m cracking up .\n",
      "< i m still on . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'not', 'competitors', 'partners', 'partners .', 're competitors', 'we re', 'not partners', 'competitors not']\n",
      "= we re competitors not partners .\n",
      "< we re not . <EOS>\n",
      "\n",
      "> ['farmer', '.', 'm', 'i', 'a', 'm a', 'farmer .', 'i m', 'a farmer']\n",
      "= i m a farmer .\n",
      "< i m a farmer . <EOS>\n",
      "\n",
      "> ['re', 'terrified', '.', 'they', 'you', 'of', 'of you', 'you .', 'terrified of', 'they re', 're terrified']\n",
      "= they re terrified of you .\n",
      "< they re terrified of you . <EOS>\n",
      "\n",
      "> ['are', 'you', 'early', '.', 'early .', 'you are', 'are early']\n",
      "= you are early .\n",
      "< you are early . <EOS>\n",
      "\n",
      "> ['re', 'to', '.', 'we', 'going', 'fight', 're going', 'fight .', 'we re', 'going to', 'to fight']\n",
      "= we re going to fight .\n",
      "< we re going to fight . <EOS>\n",
      "\n",
      "> ['he', '.', 'grouch', 's', 'a', 'a grouch', 'grouch .', 'he s', 's a']\n",
      "= he s a grouch .\n",
      "< he s a tennis . <EOS>\n",
      "\n",
      "> ['cruel', 'you', 're', '.', 'cruel .', 'you re', 're cruel']\n",
      "= you re cruel .\n",
      "< you re cruel . <EOS>\n",
      "\n",
      "> ['they', 're', 'special', '.', 'special .', 'they re', 're special']\n",
      "= they re special .\n",
      "< they re special . <EOS>\n",
      "\n",
      "> ['re', 'alone', 'anymore', '.', 'you', 'not', 're not', 'anymore .', 'you re', 'not alone', 'alone anymore']\n",
      "= you re not alone anymore .\n",
      "< you re not alone anymore . <EOS>\n",
      "\n",
      "> ['he', 'grown', 'man', '.', 's', 'a', 's a', 'grown man', 'a grown', 'he s', 'man .']\n",
      "= he s a grown man .\n",
      "< he s a it . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'speechless', 'we re', 're speechless', 'speechless .']\n",
      "= we re speechless .\n",
      "< we re being . <EOS>\n",
      "\n",
      "> ['chosen', 'the', '.', 'are', 'you', 'one', 'chosen one', 'you are', 'are the', 'the chosen', 'one .']\n",
      "= you are the chosen one .\n",
      "< you are the chosen one . <EOS>\n",
      "\n",
      "> ['stock', 're', '.', 'we', 'out', 'of', 'stock .', 'out of', 'we re', 'of stock', 're out']\n",
      "= we re out of stock .\n",
      "< we re out of dogs . <EOS>\n",
      "\n",
      "> ['.', 'm', 'understanding', 'i', 'anything', 'not', 'anything .', 'i m', 'not understanding', 'understanding anything', 'm not']\n",
      "= i m not understanding anything .\n",
      "< i m not tall . <EOS>\n",
      "\n",
      "> ['outgoing', 'he', '.', 'is', 'outgoing .', 'he is', 'is outgoing']\n",
      "= he is outgoing .\n",
      "< he is outgoing . <EOS>\n",
      "\n",
      "> ['.', 'm', 'losing', 'i', 'weight', 'm losing', 'losing weight', 'i m', 'weight .']\n",
      "= i m losing weight .\n",
      "< i m losing weight . <EOS>\n",
      "\n",
      "> ['re', '.', 'we', 'not', 'competitors', 'partners', 'partners .', 're competitors', 'we re', 'not partners', 'competitors not']\n",
      "= we re competitors not partners .\n",
      "< we re not . <EOS>\n",
      "\n",
      "> ['they', 'early', 're', '.', 'early .', 'they re', 're early']\n",
      "= they re early .\n",
      "< they re early . <EOS>\n",
      "\n",
      "> ['lawyer', '.', 'm', 'i', 'new', 'your', 'i m', 'lawyer .', 'new lawyer', 'your new', 'm your']\n",
      "= i m your new lawyer .\n",
      "< i m disgusted . <EOS>\n",
      "\n",
      "> ['.', 'm', 'i', 'persuaded', 'not', 'not persuaded', 'i m', 'm not', 'persuaded .']\n",
      "= i m not persuaded .\n",
      "< i m not deaf . <EOS>\n",
      "\n",
      "> ['pretty', '.', 'm', 'i', 'not', 'pretty .', 'not pretty', 'i m', 'm not']\n",
      "= i m not pretty .\n",
      "< i m not disagreeing . <EOS>\n",
      "\n",
      "> ['fifties', 'he', 'his', '.', 's', 'in', 'his fifties', 'in his', 'fifties .', 's in', 'he s']\n",
      "= he s in his fifties .\n",
      "< he s in his . <EOS>\n",
      "\n",
      "> ['in', 'he', '.', 's', 'fluent', 'japanese', 'japanese .', 'in japanese', 's fluent', 'fluent in', 'he s']\n",
      "= he s fluent in japanese .\n",
      "< he s in trouble . <EOS>\n",
      "\n",
      "> ['forgiven', 'you', 're', '.', 'forgiven .', 'you re', 're forgiven']\n",
      "= you re forgiven .\n",
      "< you re forgiven . <EOS>\n",
      "\n",
      "Test score using ROGUE:  0.9363999441999987\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score using ROGUE: \", evaluateDatasetWithScore(encoder1, decoder1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task - Sentence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting sentence length...\n",
      "[['re', 'talking', '.', 'we', 'done', 'we re', 'talking .', 're done', 'done talking'], 5]\n",
      "[['re', '.', 'we', 'soldiers', 'not', 'soldiers .', 'we re', 're not', 'not soldiers'], 5]\n"
     ]
    }
   ],
   "source": [
    "def prepSentLenData(trainset, testset):\n",
    "    data = []\n",
    "    newtrain = []\n",
    "    newtest = []\n",
    "    \n",
    "    print(\"Counting sentence length...\")\n",
    "    for pair in trainset:\n",
    "        #input_lang.addSentence(pair[0])\n",
    "        uwords = [t.text for t in nlp(str(pair[1]))]\n",
    "        newpair = [pair[0], len(uwords)]\n",
    "        newtrain.append(newpair)\n",
    "    \n",
    "    for pair in testset:\n",
    "        uwords = [t.text for t in nlp(str(pair[1]))]\n",
    "        newpair = [pair[0], len(uwords)]\n",
    "        newtest.append(newpair)\n",
    "        \n",
    "    return newtrain, newtest\n",
    "\n",
    "sentLenTrainData, sentLenTestData = prepSentLenData(train_set, test_set)\n",
    "print(random.choice(sentLenTrainData))\n",
    "print(random.choice(sentLenTestData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the multi-level perceptron net for sentence length classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, class_size=6, act='Tanh'):\n",
    "        super(MLPNet,self).__init__()\n",
    "        \n",
    "        self.hids = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(100, 64)\n",
    "        self.tanh = eval('nn.{}'.format(act))()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, class_size)\n",
    "        \n",
    "        self.hid_modules = nn.ModuleList([h[0] for h in self.hids])\n",
    "\n",
    "        #self.classifier = nn.Linear(options['n_hid'], 2)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, sentEmb):\n",
    "\n",
    "        h = sentEmb\n",
    "        h = self.fc1(h)\n",
    "        h = self.tanh(h)\n",
    "        h = self.fc2(h)\n",
    "        h = self.tanh(h)\n",
    "        h = self.fc3(h)\n",
    "        \n",
    "        #z = self.classifier(h)\n",
    "        m = nn.Softmax(dim=1)\n",
    "        z = m(h)\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSentLen(net, encoder, train, test, class_size=6, n_epochs = 100, act='Tanh'):\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    input_batches = []\n",
    "    label_batches = []\n",
    "    plot_losses = []\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print('Starting the {}-th epoch..'.format(epoch))\n",
    "        permutation = torch.randperm(len(train))\n",
    "        #print(permutation)\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        label_onehot = torch.LongTensor(batch_size, class_size)\n",
    "        \n",
    "        for i in range(0,len(train), batch_size):\n",
    "            \n",
    "            #print('#', end='')\n",
    "            \n",
    "            indices = permutation[i:i+batch_size]\n",
    "            input_batch = []\n",
    "            label_batch = []\n",
    "            \n",
    "            for j in indices:\n",
    "                input_variable = variableFromNGramList(vocab_ngrams, train[j][0], ORDER, NUM_WORDS)\n",
    "                input_length = input_variable.size()[0]\n",
    "                \n",
    "                encoder_hidden = encoder.initHidden()\n",
    "                encoder_outputs = Variable(torch.zeros(MAX_LENGTH*ORDER, encoder.hidden_size))\n",
    "                encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "                \n",
    "                for ei in range(input_length):\n",
    "                    encoder_output, encoder_hidden = encoder(\n",
    "                        input_variable[ei], encoder_hidden)\n",
    "                    encoder_outputs[ei] = encoder_output[0][0]\n",
    "                input_batch.append(encoder_hidden.view(100))\n",
    "                label_batch.append(train[j][1] - 3)\n",
    "                    \n",
    "            input_batch = torch.stack(input_batch)\n",
    "            label_batch = torch.LongTensor(label_batch).view(128,1)\n",
    "            \n",
    "            label_onehot.zero_()\n",
    "            label_onehot.scatter_(1, label_batch, 1)\n",
    "            \n",
    "            out = net(input_batch).float()\n",
    "            #print(out)\n",
    "            loss = net.criterion(out, Variable(torch.max(label_onehot, 1)[1]))\n",
    "            net.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "           \n",
    "        print_loss_avg = print_loss_total / batch_size\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        plot_loss_avg = plot_loss_total / batch_size\n",
    "        plot_losses.append(plot_loss_avg.data[0])\n",
    "        plot_loss_total = 0\n",
    "    \n",
    "    #print(plot_losses)\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the 1-th epoch..\n",
      "1m 9s (- 10m 28s) (1 10%) 0.5206\n",
      "Starting the 2-th epoch..\n",
      "2m 9s (- 8m 39s) (2 20%) 0.4717\n",
      "Starting the 3-th epoch..\n",
      "3m 15s (- 7m 37s) (3 30%) 0.4393\n",
      "Starting the 4-th epoch..\n",
      "4m 18s (- 6m 27s) (4 40%) 0.4211\n",
      "Starting the 5-th epoch..\n",
      "4m 59s (- 4m 59s) (5 50%) 0.4098\n",
      "Starting the 6-th epoch..\n",
      "6m 3s (- 4m 2s) (6 60%) 0.4011\n",
      "Starting the 7-th epoch..\n",
      "6m 44s (- 2m 53s) (7 70%) 0.3947\n",
      "Starting the 8-th epoch..\n",
      "7m 48s (- 1m 57s) (8 80%) 0.3889\n",
      "Starting the 9-th epoch..\n",
      "8m 28s (- 0m 56s) (9 90%) 0.3842\n",
      "Starting the 10-th epoch..\n",
      "9m 34s (- 0m 0s) (10 100%) 0.3799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12acb3160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128a644e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = (3, 4, 5, 6, 7, 8)\n",
    "batch_size = 128\n",
    "net = MLPNet(len(classes))\n",
    "trainSentLen(net, encoder1, sentLenTrainData, sentLenTestData , n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Length on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSentLenTestSet(net, encoder, testset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for pair in testset:\n",
    "        input_variable = variableFromNGramList(vocab_ngrams, pair[0], ORDER, NUM_WORDS)\n",
    "        input_length = input_variable.size()[0]\n",
    "                \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH*ORDER, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "                \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                input_variable[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0][0]\n",
    "        \n",
    "        #print(encoder_hidden)\n",
    "        \n",
    "        outputs = net(encoder_hidden.view(1,100))\n",
    "        #print(outputs)\n",
    "        #print(torch.max(outputs, 1)[1].data[0])\n",
    "        predict = torch.max(outputs, 1)[1].data[0]\n",
    "        \n",
    "        total += 1\n",
    "        if (predict+3 == pair[1]):\n",
    "            correct += 1\n",
    "\n",
    "    print('Accuracy of the network on the sentence length test set: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the sentence length test set: 75 %\n"
     ]
    }
   ],
   "source": [
    "evaluateSentLenTestSet(net, encoder1, sentLenTestData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attn_decoder1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ccecfdde33ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m output_words, attentions = evaluate(\n\u001b[0;32m----> 2\u001b[0;31m     encoder1, attn_decoder1, \"je suis trop froid .\")\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attn_decoder1' is not defined"
     ]
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human â†’ Machine (e.g. IOT commands)\n",
    "   -  Chat â†’ Response\n",
    "   -  Question â†’ Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 0\n",
      " 5\n",
      " 9\n",
      " 3\n",
      "[torch.LongTensor of size 5x1]\n",
      "\n",
      "\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 5x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 5\n",
    "nb_digits = 10\n",
    "# Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
    "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
    "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
    "\n",
    "# In your for loop\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "print(y)\n",
    "print(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
